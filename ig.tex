\subsection{Interaction Grammars}

We will briefly discuss one more grammar formalism and that is the
formalism of Interaction Grammars \cite{guillaume2009interaction}.
Interaction grammars are a grammatical formalism centered around the
concept of \emph{polarity}. An interaction grammar is a set of
(underspecified) tree descriptions which define a set of trees that can
be constructed by superposing some of these tree descriptions while
respecting the polarities described in the nodes of the individual tree
descriptions.

We will illustrate the objects and mechanisms of interaction grammars on
a simple example sentence.

\begin{exe}
  \ex \label{ex:ig} Jean la voit.
\end{exe}

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{ig-parse.pdf}
  \caption{\label{fig:ig-parse} Syntactic tree of sentence
    (\ref{ex:ig}). The numbers at the top of the nodes are labels of
    nodes from the elementary polarized tree descriptions that generated
    the syntactic tree (see Figure \ref{fig:ig-eptds}).}
\end{figure}

Figure \ref{fig:ig-parse} shows the syntactic tree assigned by Frigram
to the sentence in (\ref{ex:ig}). It is a rooted ordered tree with the
topmost node being the root. Solid lines indicate immediate dominance
with the higher node being the parent and the lower the child. The
ordering of children is given by the arrows, which signify the immediate
precedence relation between sister nodes.

The nodes in the tree are of three different kinds with respect to the
phonological form of their subtree. We recognize the \emph{anchor
  nodes}, which are displayed in vivid yellow and which are leaf nodes
containing some non-empty string as their phonological content (written
in a gray rectangle at the top of the node). Then we have the
\emph{empty} nodes, which are drawn in white and whose phonological form
is empty. Finally, there are the pale yellow \emph{non-empty} nodes,
internal nodes whose phonological content (the yield of the subtree) is
not empty.

The nodes of the syntactic tree are also decorated with
features.

Interaction grammars are a formalism enabling us to define sets of the
structures described above, the \emph{syntactic trees}. Similar to Tree
Adjoining Grammars, interaction grammars are formulated in terms of a
set of elementary structures which can combine to produce the final
output structures. However, unlike in tree adjoining grammars, the
output structure is not constructed from the elementary structures via
some set of algebraic operations (substitution and adjunction in case of
TAG). Instead, \emph{polarized tree descriptions} (PTDs), the elementary
structures of interaction grammars, impose constraints on the final
structure and a structure is said to be generated by some PTDs if it is
a minimal structure satisfying those constraints (it is a \emph{minimal
  model}).

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{ig-eptds.pdf}
  \caption{\label{fig:ig-eptds} The elementary polarized tree
    descriptions used to generate the sentence (\ref{ex:ig}).}
\end{figure}

In Figure~\ref{fig:ig-eptds}, we can see the elementary polarized tree
descriptions (EPTDs) that generated the parse tree in
Figure~\ref{fig:ig-parse}.

As we said before, a PTD is a set of constraints on some syntactic
tree. Let us expound on what constraints the structures of
Figure~\ref{fig:ig-eptds} impose.

A node in a PTD can be read as a statement that there must exist a node
in the final syntactic tree that has compatible values for all the
features and that carries the same phonological string, if any. Such a
node of the syntactic tree is then called the \emph{interpretation} of
the PTD node (in Figure~\ref{fig:ig-parse}, every node of the syntactic
tree bears a list of the PTD nodes that it interprets, so you can see
exactly how the interpretation function works in our example).

A solid line between two nodes in a PTD means that the interpretations
of these two nodes must be in a parent-child relationship
(\emph{immediate dominance}). A dashed green arrow tells us that the
interpretations have to be sister nodes with the former preceding the
latter in the ordered tree (\emph{precedence}). Note that not all sister
nodes in a PTD have to be linked with this precedence relation. See for
example the nodes (1,1) and (1,5) in the EPTD of the clitic pronoun
$la$.

The formalism also allows us to specify \emph{immediate precedence} and
(large) \emph{dominance}. The latter is useful for modelling unbounded
dependencies such as those between a relative pronoun and its trace in
some embedded clause of the relative clause, but it is not used in our
present example.

Finally, one kind of constraint that is used in our example is the
orange rectangle in node (3,0), which requires that the interpretation
of (3,0) is the rightmost daughter amongst its siblings.

Now that we have covered the structural constraints imposed by the tree
descriptions, we will turn our attention to the defining characteristic
of interaction grammars, the polarities. As you have noticed on
Figure~\ref{fig:ig-eptds}, some of the features in our PTDs are
annotated with special symbols and colors. These denote the different
polarities and are used by the formalism for two distinct purposes: the
positive ($\textcolor{red}{\rightarrow}$) and negative
($\textcolor{blue}{\leftarrow}$) polarities are used to model the
resource sensitivity of languages, while the virtual polarities
($\textcolor{purple}{\sim}$) are used to for pattern matching against
the context.

The way the polarities are handled is that every model (output syntactic
tree) is required to have only saturated polarities on its features
(i.e. no positive, negative or virtual polarities). Whenever more than
one node of the PTD is mapped onto a single node of the syntactic tree,
the polarities of each feature are combined. The combination mechanism
allows us to combine a positively polarized feature with a negatively
polarized one to yield a saturated one. Virtual polarities can only be
eliminated by combining them with a saturated polarity (they are
analogous to the $=_{c}$ constraints of LFG \cite{kaplan1982lexical}).

If we look at the PTDs of Figure~\ref{fig:ig-eptds}, we can see this
polarity mechanism in action. The EPTD of $Jean$ has a positive $cat$
feature in its root node saying that it provides one $np$, and a
negative $funct$ feature saying that it expects some function. In the
EPTD of $voit$, we have two nodes for the subject and object, both of
them expecting an $np$ and providing them the $subj$ and $obj$
functions, respectively. The root of the EPTD then provides a complete
sentence of category $s$ and expects some function that this sentence
will play. The full stop EPTD finalizes the sentence by accepting a node
with category $s$ and giving it a $void$ function.

The clitic pronoun $la$ participates in the positive/negative resource
management system as well, since using the clitic fills up the object
slot in the valency of a verb. The EPTD of $la$ also relies heavily on
virtual features and uses them to specify in more detail the context in
which this rule is applicable, so as to avoid over-generation.

Now that we understand the constraints imposed by PTDs, we can start to
see that the syntactic tree in Figure~\ref{fig:ig-parse} is truly a
model of the PTDs given in Figure~\ref{fig:ig-eptds} and more than that,
it is the only minimal model. To illustrate more clearly how the
polarities and constraints of the individual PTDs end up generating the
syntactic tree of Figure~\ref{fig:ig-parse}, we can look at the result
of superposing the EPTDs of $la$ and $voit$ by merging the nodes (1,2)
and (1,5) with (2,2) and (2,4) respectively on
Figure~\ref{fig:ig-partial}.

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{ig-partial.pdf}
  \caption{\label{fig:ig-partial} The result of merging the EPTDs of
    $la$ and $voit$ by merging the nodes (1,5) and (1,5) with (2,2) and
    (2,4), respectively.}
\end{figure}

\subsubsection{Frigram, an interaction grammar}

The reason we are interested in interaction grammars is because of the
existence of Frigram, a large scale grammar of French which is
lexicalized by Frilex, an independent lexical resource that we will be
building our grammar on as well. Furthermore, as we will see
in~\ref{sssec:link-ig-acg}, interaction grammars are closely linked with
abstract categorial grammars, which makes Frigram a suitable grammar to
use as a guide when developing our grammar.

In this section, we will introduce the metagrammatical structure of
Frigram and talk about how it uses the formalism of interaction grammars
to solve several tricky linguistic phenomena.


Frigram is a wide-coverage interaction grammar of French. As we
mentioned before, an interaction grammar is a set of elementary
polarized tree descriptions. In a lexicalized interaction grammar, each
of these EPTDs is connected to a specific wordform whose particular use
it describes. Given the amount of wordforms that a useable grammar of
French would need to cover, the number of EPTDs in our grammar could
easily reach hundreds of thousands, if not millions.

The first step in fighting this explosion is to factor out irrelevant
differences between similar wordforms. For this purpose, Frilex was
created. Frilex is a morpho-syntactic lexicon of French compiled from
various other pre-existing lexicons of French. It is in effect a large
relation linking the wordforms of French to \emph{hypertags}, feature
structures describing the morphological properties of the wordforms and
their syntactic valencies.

With Frilex in place, Frigram can be defined as a set of unanchored
EPTDs which are paired with feature structures which delimit the subset
of the Frilex items to which the EPTDs apply. This kind of simple
``metagrammar'' already saves us a lot of effort, the number of
unanchored EPTDs defined by Frigram is somewhere around 4000 (TODO: Cite
the Frig documentation somehow).

However, defining some 4000 EPTDs manually still seems like a very
tedious, error prone and hard to maintain approach. Many of the EPTDs
have to repeatedly describe common phenomena such as subject-object
agreement or predicate-argument saturation. Similarly as in other
software endeavors, it would be preferable to define these common
patterns in some reusable module and compose EPTDs from these building
blocks. This is where XMG steps in.

XMG is a metagrammar compiler adding yet another level of indirection
between what the grammar author writes and what ends up in the
bottom-level interaction grammar. XMG provides a language that lets the
grammar author define the lexical items of his grammar and to combine
these definitions to yield new and more elaborate lexical items.

In the case of Frigram, the lexical items (termed \emph{classes} in XMG)
take the shape of PTDs coupled with feature structures which
circumscribe the interface to Frilex. Both tree descriptions and feature
structures have very natural ways of composing together, by conjunction
and unification, respectively. This, alongside with disjunction, are the
chief tools that the grammar authors can use (and in the case of
Frigram, have used) to succintly define their grammar.

With the added capability of composing classes, the definition of
Frigram reduces to about 400 class definitions. Of these classes, 160
are terminal, meaning that they actually describe lexical items that are
to be included in the final grammar. Thanks to the disjunction
composition operator of XMG, these 160 classes end up generating the
staggering 4000 EPTDs.


\begin{figure}
  \centering
  \includegraphics[scale=0.25]{ig-neg.pdf}
  \caption{\label{fig:ig-neg} EPTDs of $aucun$ and $ne$ demonstrating
    the way polarities are used to model French negation.}
\end{figure}

We will briefly discuss how interaction grammars, and Frigram in
particular, handle a tricky linguistic phenomenon in an interesting way.
In French, negation is signalled by the particle $ne$ which must be
accompanied by one of several designated determiners, pronouns or
adverbs. The particle $ne$ assumes a position right before the inflected
verb, but its partner, e.g. the determiner $aucun$ can appear in a rich
variety of positions:

\begin{exe}
  \ex \label{ex:aucun-suj} Aucun tatou ne court.
  \ex \label{ex:aucun-obj} Jean n'aime l'odeur d'aucun tatou.
  \ex \label{ex:aucun-bad} * Le tatou qu'aucun loup chasse ne court.
  \ex \label{ex:aucun-good} Le tatou qu'aucun loup ne chasse court.
\end{exe}

Sentences (\ref{ex:aucun-suj}) and (\ref{ex:aucun-obj}) demonstrate that
the $aucun$ determiner can be used both in the subject and object
positions, as a determiner of either one of the arguments directly or of
a noun phrase which complements one of them. However, it is not
admissible to place the determiner in an embedded clause as it is in
(\ref{ex:aucun-bad}). If the determiner $aucun$ is used in an embedded
clause, it is the embedded clause itself that must be negated, as in
sentence (\ref{ex:aucun-good}).

This example demonstrates two points of interest to us.

First, it uses a new polarized feature, $neg$, to model the fact that
the two wordforms which enable negation must always co-occur. The
particle $ne$ provides a positive $neg$ feature to the clause in which
it occurs and this positive polarity must be saturated by a negative
polarity contributed by one of the possible partner words, such as
$aucun$. This shows that the usefulness of positive and negative
polarities extends beyond the $cat$/$funct$ pair that we have already
seen and which is used to model predicate/argument structure in a
similar way as was already done in tree adjoining grammars or categorial
grammars.

Second, the determiner $aucun$ can occur deep, e.g. inside a complement
of one of the arguments, but it cannot occur everywhere, as is
demonstrated by (\ref{ex:aucun-bad}), where we try to put the determiner
inside another embedded clause. There is a way to express this kind of
constraint in interaction grammars that we have not revealed
yet. Whenever we use a (large) dominance (a top-down green dashed line),
we usually want the range of nodes that this dominance relation can
cross to be somehow restricted. In interaction grammars, we can
associate a feature structure with a dominance constraint which will
force all the nodes that will span the distance between the two nodes
linked by the dominance constraint to unify with that feature
structure.

In the notation used in our illustrations (which were generated by the
Leopar parser\footnote{\url{http://leopar.loria.fr/}}), this is conveyed
by putting all the restricting features as virtual features in a new
node and splicing the new node in between the two nodes linked by the
dominance constraint before (see the node (0,4) in
Figure~\ref{fig:ig-neg}, which merely serves to ensure that all the
nodes on the path from (0,3) to (0,2) have either $np$ or $pp$ as the
value of their $cat$ feature). When using this notation, the (large)
dominance constraint should then be read as not only stating that the
top node must be an ancestor to the bottom node, but also that any
intermediate nodes between the ancestor and successor must match the
ancestor's features.

\subsubsection{The link with abstract categorial grammars}
\label{sssec:link-ig-acg}
