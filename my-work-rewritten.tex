\section{Wide-coverage abstract categorial grammars}

Our goal is to develop large abstract categorial grammars which cover a
variety of linguistic phenomena. All of these phenomena have been
studied in detail but we would like to have a grammar which gracefully
combines all these solutions. At this scale, working out the grammar on
paper stops being sufficient to detect all the subtle interactions and
it is desirable to have a formalized representation that can be reasoned
about or tested by a computer.

To some extent, this need is already met by the \textbf{ACG development
  toolkit}\footnote{\url{http://www.loria.fr/equipes/calligramme/acg/}}. The
toolkit reads in signature and lexicon definitions and offers two modes
of operation. In the first, it checks the validity of the defined
ACG. That is to say, it checks whether the terms assigned by the
lexicons are well-typed and whether their types are consistent with the
object types dictated by the lexicon. The second mode of operation
offers an interactive experience in which the user is free to run type
inference on terms built on one of their signatures to find out whether
they are well-typed and if so, what is their principal type. The
interactive mode then lets the user apply a lexicon, or a composition of
lexicons, to a term and get the $\beta$-normal form of the object term.

We consider the above capabilities vital for doing any involved grammar
engineering in the framework of ACGs. However, there are some
limitations which make it not sufficient for our needs. The foremost of
those is that the signatures and lexicons are all explicitly realized in
(primary) memory and the toolkit expects them to be given directly in a
file. The grammars that we would like to develop will cover an
exhaustive list of wordforms and the size of our grammar definitions
would therefore grow proportionately with the size of our dictionary.

Given this setup, there are several approaches we might try, none of
them too appealing. We might write our metagrammar in a different format
and use a tool which would combine the metagrammar and the lexical
database\footnote{We will be using the term \emph{lexical database} to
  refer to what is usually called a lexicon, in order not to cause
  confusion with the lexicons of ACGs.} to produce the lexicalized ACG
in a direct representation, ready to be loaded by the toolkit. This
workflow would make development on the grammar tedious as every time we
would change the metagrammar, we would have to regenerate the direct
representation and then load it in the ACG toolkit. With extra effort,
the former cost could be alleviated by devising a system for
regenerating only parts of the grammars which will have changed since
last time. However, we would still have to pay the price of loading the
entire grammar into the toolkit before being able to interact with it
again.

Furthermore, we could pick a small representative fragment of the
dictionary and test only on that fragment during the development of our
grammar, to reduce the time it takes to both generate the direct
representation of the grammar and the time it takes to load it in the
toolkit. This would have made developing the grammar already
tenable.

However, in our approach, we have opted to spend more time on tool
development to ease subsequent work and we have developed a system which
makes defining and experimenting with lexicalized ACGs easier. The
metagrammars are defined as relational procedures which link the items
of the lexical database to elements of signatures and lexicons. The
grammars can be redefined without having to reload the lexical database
or any file of similar scale. Type inference and lexicon application
with $\beta$-normalization are provided as relations which can be used
interactively or as part of a larger program. En example of such a
larger program might be a routine to check the validity of an ACG or to
test some of its properties.

In this chapter, we will spend time explaining the decisions which went
into designing the system and the form it has now.

\subsection{The tools and techniques}

The dominant decision which we made during the development of the system
was to use \emph{relational programming}. Relational programming is a
discipline of logic programming where relations are written in a
``pure'' style \cite{byrd2010relational}. This means that relations
cannot be picky about which arguments are passed in as inputs and which
as outputs, all modes must be permissible. Accepting such a discipline
prohibits us from using some of the special operators of Prolog such as
cut (\texttt{!}) and \texttt{copy\_term/2}. Instead, we must resort to
other techniques which do not break purity, like \emph{constraint logic
  programming}, which allows us to express properties outside of the
scope of unification by attaching constraints to terms and verifying
them at the proper time.

By insisting on purity, I/O becomes more difficult as we would not allow
the side-effectful pseudorelations of Prolog. Therefore, we will not
force ourselves to implement the entire program front-to-back using only
relational programming. Instead, we will use a relational programming
system, miniKanren, which is embedded inside a functional programming
language. This lets us reap the benefits of relational programming
inside the core logic of our program while deferring to the functional
programming language to provide I/O and other bookkeeping operations.

\subsubsection{The case for relational programming}

There are several reasons why we might want to choose a relational
programming system for our implementation. First off, the problems which
consiture the algorithmic core of our program are often mere textbook
examples of how to use a relational programming system. This goes for
type inference/checking/inhabitation, which are all implemented by a
single relation which is often used as a kind of ``Hello World'' program
in the world of relational programming. Similarly, implementing
$\beta$-reduction cleanly (without having to handle variable clashes by
generating new variables and $\alpha$-converting) is a motivational
example for introducing \emph{nominal logic programming}. To reach
feature parity with the ACG toolkit, one would only need to implement
the mapping of a lexicon over a term, which is a simple functor.

The use of logic programming techniques is also very popular in our
domain. If we look at the system demonstrations at the \emph{Logical
  Aspects of Computational Linguistics 2012} conference, we see that 3
out of the 5 proposals have been implemented in a declarative/logic
programming system (Oz, Prolog). 2 of the 3 systems are parsers for
categorial grammars (automated theorem provers), which might turn out to
be an interesting direction for our system to go into as well, as
practical applications appear on the horizon.\footnote{Theorem provers
  are another problem area where miniKanren fits well, see
  $\alpha$leanTAP \cite{near2008alpha}.}

The third system was XMG, a language for writing metagrammars, which is
another goal of our system, the one which distinguishes it from the
original ACG toolkit. We believe that using a declarative programming
system such as miniKanren with the ability to extend it using a
practical functional programming language presents an empowering way of
defining metagrammars. This allows the grammar designer to compose his
grammar in any way he sees fit and to use the metaprogramming facilities
of the underlying programming language to transcribe the grammar in a
way that makes the most sense in her case.

Finally, some of the structures that we will encounter in our domain are
more directly modelled in mathematics as a relation instead of a
function. This has the benefit that to implement a relation in
relational programming, one just writes the relation, while if we were
forced to encode everything in functions, we would have to manage this
multiplicity of values manually. Examples of structures that have this
kind of structures if Frilex, our lexical database, where a single
wordform can be mapped to multiple hypertags. We will see another
example of a relation within our domain in the next subsection and we
will also discuss the advantages and drawbacks of modelling functions as
relations.

\subsubsection{Choice of implementation language}

There are two languages which play host to a substantial implementation
of miniKanren (meaning one that has extensible unification and/or
constraints and that includes support for nominal logic),
Racket\footnote{\url{https://github.com/calvis/cKanren}} and
Clojure\footnote{\url{https://github.com/clojure/core.logic}}. The two
languages are very similar to each other and therefore the choice is of
no great significance. An appealing feature of Clojure is that it uses
abstract collection types pervasively instead of tying the standard
operations to specific structures such as lists and cons cells and that
it follows other similarly enlightened design decisions. Racket on the
other hand has had more time to mature, has better metaprogramming
facilities and is more recognized in academia. In developing our system,
we have settled for Clojure. One of the advantages of doing so is that
the Clojure implementation of miniKanren, dubbed core.logic, has
inspired the software community and the project has attracted
contributors that submit issue reports and
patches\footnote{\url{http://dev.clojure.org/jira/browse/LOGIC}}.


\subsection{Defining signatures and lexicons}

\subsubsection{Defining signatures}

In~\ref{sssec:sig}, we formally presented signatures as triples
$\mathopen{<}A, C, \tau\mathclose{>}$. The set $C$ can be easily
reconstructed as the domain of $\tau$ and the set $A$ is just the set of
all atomic types occurring in the range of $\tau$. Therefore, to define
the signature $\mathopen{<}A, C, \tau\mathclose{>}$, it is enough to
provide the function $\tau$, which in its set-theoretical realization is
just a set of pairs. In relational programming, a set can be easily
encoded as a unary relation that answers the membership test. Therefore,
our goal will be to construct to answer this relation and this relation
will be our final representation of a signature.

We have our lexical database, a set of pairs of wordforms and hypertags,
and we want to arrive at a set of pairs of constants and their
types. This mapping decomposed into smaller parts by considering the
contribution of every part of the lexical database individually. What
will be the right model for the link between some part of the lexical
database and some part of our signature?

\begin{description}
  \item[Function of wordform] We could imagine that every different
    wordform of the lexical database will end up being mapped to some
    constant in the signature. However, this is unsuitable since the
    lexical database is a relation that can assign more than one
    distinct hypertag to a single wordform (e.g. with different part of
    speech) and we would like to have constants of different types for
    each of these hypertags.
  \item[Function of hypertag] Conversely, we might imagine that the
    hypertags of the lexical database are mapped via a function to the
    constants of the signature. Here we run into trouble when we
    consider variations in spelling or phonology. We might have two
    items in our lexical database that share the same hypertag but whose
    wordforms still differ. We would like to have two distinct constants
    in our signature then, so that when these two constants are mapped
    by the surface lexicon, they will yield two different strings.
  \item[Function of lexical entry] Since neither the wordform nor the
    hypertag is enough, we could consider the entire lexical entry (an
    element of the lexical database, a pair of a wordform and one of its
    hypertags). This solves both of the above problems but it is still
    not perfect. Consider the case of a determiner and some semantic
    signature. Since our link between elements of the lexical database
    and of our signature is a function, there must be a constant in the
    signature for every item of our lexical database. If you recall
    in~\ref{sssec:acg-sem}, the constants of the semantic signature
    $\Sigma_{Sem}$ contained predicates for the common nouns $man$ and
    $woman$ and for the verb $loves$ and some common logical
    furniture. However, it did not contain any constants which would
    correspond to the determiners $every$ and $some$.
  \item[Partial function of lexical entry] We could fix the issue above
    by using a partial function. This accounts for cases when a lexical
    entry has no constants to represent it in a signature. Nevertheless,
    this only fixes half of the problem. We will consider the single
    lexical entry which represents e.g. the 3rd person singular present
    tense form $loves$ of the transitive verb $love$. A legitimate
    account of subject/object scope ambiguity would be to consider two
    constants in our abstract syntactic signature that have distinct
    images in some semantic lexicon, one giving the outer scope to the
    subject and the other to the object. This means that in order to
    define such a signature, we would need to link a single lexical
    entry in our database to more than one constant in the signature.
  \item[Relation between lexical entry and constant] Finally,
    considering all the above common scenarios, the only mathematical
    model general enough to handle all of the above cases is that of a
    relation. In our system, this will correspond exactly to a relation
    that will be provided by the grammar author.
\end{description}

We have established that lexicons will be defined by a relation, a
relation between a lexical entry (a wordform and a hypertag) and a
constant (an identity\footnote{The identity of a constant is simply what
  distinguishes it from other constants of the same signature having the
  same type.} and a type). The identity of a lexical constant also has a
complex structure. We know that the lexical constants we define using
the relation described above are always linked to a lexical entry. This
lexical entry at least partially identifies the constant. For cases
where we have more than one constant per lexical entry (such as the
example of the two different scope readings of $loves$), we need some
extra piece of information to specify which constant we are referring to
(we call this extra piece of information a \emph{specifier}). This
specifier can have any shape that the grammar author desires. We can see
an example of a lexical constant in Figure~\ref{fig:lex-const}.

\begin{figure}
  % TODO: Center this.
  \centering
\begin{verbatim}
{:type [-> NP [-> NP S]]
 :id {:lex-entry {:wordform "loves"
                  :hypertag ...}
      :spec {:scope :subject-wide}}}
\end{verbatim}
  \caption{\label{fig:lex-const} An example of a lexical constant from a
    hypothetical syntactic signature (the hypertag is elided for brevity).}
\end{figure}

The system takes care of wiring up the relations and assembling the
signature. The grammar author's duty is to provide a 4-ary relation that
links the four parts of a lexical constant: the wordform, the hypertag,
the specifier and the type. This relation can be thought of as a
multi-valued ``function'' that receives as input a wordform and a
hypertag (that is, a single lexical entry) and outputs (possibly
multiple) pairs of specifiers and types. Since we are working in a
functional programming language that supports manipulation of
higher-order functions, we can provide helper functions which construct
these 4-ary relations for the grammar author in some simple cases. Our
library thus provides functions like \texttt{unitypedr}, which expects a
type and returns the 4-ary relation that assigns to every lexical item a
single constant whose specifier is $nil$ and whose type is the supplied
type, or \texttt{ht->typer}, which expects a mapping from hypertag
patterns to types and returns the 4-ary relation which tries to map the
hypertag against the patterns and yields constants with the
corresponding types and $nil$ specifiers. When other patterns emerge,
the grammar author is completely free to implement her new abstractions
using the full power of the functional programming language. See
Figure~\ref{fig:lex-sig-impl} for an example of implementing a
lexicalized version of the $\Sigma_{Synt}$ signature
from~\ref{sssec:example-sig} using the \texttt{ht->typer} abstraction.

\begin{figure}
  \centering
\begin{verbatim}
(ht->typer {{:head {:cat "n"}}      'N
            {:head {:cat "v"
                    :trans "true"}} (-> 'NP 'NP 'S)
            {:head {:cat "det"}}    (-> 'N (-> 'NP 'S) 'S)})
\end{verbatim}
  \caption{\label{fig:lex-sig-impl} A lexicalized version of the
    $\Sigma$(\_{Synt}) signature from~\ref{sssec:example-sig}.}
\end{figure}

In the above paragraphs, we have been referring to the constants that we
were defining as \emph{lexical constants}. That was to distinguish them
from the \emph{non-lexical constants} that we will introduce now. The
reason for having non-lexical constants is that not every constant of
every signature is linked to (generated by) some item of our lexical
database. Consider, for example, the empty string $\epsilon$ and the
string concatenation operator $+$ of the $\Sigma_{String}$ signature we
gave in~\ref{sssec:example-sig} or the quantifiers and logical
connectives of the $\Sigma_{Sem}$ signature in~\ref{sssec:acg-sem}.
These constants are not associated to any lexical entry or specifier and
therefore their representation differs. See
Figure~\ref{fig:nonlex-const} for an example of an non-lexicalized
constant from a semantic signature.

\begin{figure}
  \centering
\begin{verbatim}
{:type [-> T [-> T T]]
 :id {:constant-name and?}}
\end{verbatim}
  \caption{\label{fig:nonlex-const} An example of the representation of
    a non-lexicalized constant (the conjunction operator in a semantic
    signature).}
\end{figure}

Signatures of non-lexical constants can be simply defined by a mapping
from the constant names to their types as
in~\ref{fig:nonlex-sig-impl}. Such signatures will generally not be
sufficient by themselves and will need to be complemented by other
signatures which contain the lexical constants. Since we represent
signatures as unary relations, combining them to produce their unions or
intersections is just a matter of taking their disjunctions or
conjunctions, respectively. With this, we have enough to fully define
the other signatures of subsection~\ref{ssec:acg} in a lexicalized
manner, see Figure~\ref{fig:example-sig-impl}.

\begin{figure}
  \centering
\begin{verbatim}
(nonlex-sigr {'and?    (-> 'T 'T 'T)
              'imp?    (-> 'T 'T 'T)
              'forall? (-> (=> 'E 'T) 'T)
              'exists? (-> (=> 'E 'T) 'T)})
\end{verbatim}
  \caption{\label{fig:nonlex-sig-impl} A signature of non-lexical
    constants belonging to $\Sigma$(\_{Sem})
    from~\ref{sssec:acg-sem}. The single arrow \texttt{->} corresponds
    to linear implication while the double arrow \texttt{=>} corresponds
    to intuitionistic implication.}
\end{figure}

\begin{figure}
  \centering
\begin{verbatim}
(def synt-sig
  (ht->typer {{:head {:cat "n"}}      'N
              {:head {:cat "v"
                      :trans "true"}} (-> 'NP 'NP 'S)
              {:head {:cat "det"}}    (-> 'N (-> 'NP 'S) 'S)}))

(def string-sig
  (ors (nonlex-sigr {'++ (-> 'Str 'Str 'Str)})
       (unitypedr 'Str)))

(def sem-sig
  (ors (nonlex-sigr {'and?    (-> 'T 'T 'T)
                     'imp?    (-> 'T 'T 'T)
                     'forall? (-> (=> 'E 'T) 'T)
                     'exists? (-> (=> 'E 'T) 'T)})
       (ht->typer {{:head {:cat "n"}}      (-> 'E 'T)
                   {:head {:cat "v"
                           :trans "true"}} (-> 'E 'E 'T)})))
\end{verbatim}
  \caption{\label{fig:example-sig-impl} The definition of the
    lexicalized versions of the example signatures of
    subsection~\ref{ssec:acg}.}
\end{figure}

\subsubsection{Defining lexicons}

We have explained how we can use elements of our lexical database to
define signatures. Now, we will turn to lexicons. Similar to how we
reduced signatures from the formal triples $\mathopen{<}A, C,
\tau\mathclose{>}$ to just the type assignment function $\tau$, we can
similarly reduce lexicons from the pairs $\mathopen{<}F, G\mathclose{>}$
to only $G$, the part which operates on terms. $F$, or at least its
relevant subset, can be easily retrieved by putting the types of the
abstract constants side-by-side with the types of their object terms to
get a sufficient subset of $\hat{F}$ and finding $F$, the mapping from
atomic abstract types whose homomorphic extension includes $\hat{F}$.

In our relational framework, a mapping is most directly represented as a
binary relation, which is the representation that we have chosen. In the
previous subsubsection, we talked a lot about how individual entries in
the lexical database generate the items of a signature. Solving the
problem for signatures also solved it for lexicons, since their
structure is very similar: a signature is a set of pairs of constants
and types, a lexicon is a set of pairs of constants and
terms. Therefore, a lexical entry contributes the object terms for the
constants which it generated in the abstract signature.

Our representation of lexical constants was purposefully engineered so
as to make the lexical entry directly accessible to the lexicon
implementation. This way, the lexicon author can inspect the hypertag
and the wordform of the lexical entry, the specifier of the concrete
constant that it is mapping right now and even the type she assigned to
the constant in the abstract signature to allow her to choose the
correct object term. Furthermore, since lexicons are still
relations/sets, we can compose them together by disjunction. See
Figure~\ref{lex-impl} for the implementation of the lexicalized versions
of the $\mathcal{L}_{Syntax}$ and $\mathcal{L}_{Sem}$ lexicons.

\begin{figure}
  \centering
\begin{verbatim}
(defn syntax-lexo
  [synt-constant string-term]
  (with-sig-consts string-sig
    (l/fresh [hypertag string-constant]
      (has-hypertago synt-constant hypertag)
      (share-lex-entryo synt-constant string-constant)
      (string-sig string-constant)
      (fs-assigne hypertag string-term
                  {:head {:cat "n"}}
                  ,(rt string-constant)
                  {:head {:cat "v"
                          :trans "true"}}
                  ,(rt (ll [x y] (++ (++ x string-constant) y)))
                  {:head {:cat "det"}}
                  ,(ll [x R] (R (++ ua-stx-constant x)))))))

(defn sem-lexo
  [synt-constant sem-term]
  (with-sig-consts sem-sig
    (l/fresh [hypertag sem-constant]
      (has-hypertago synt-constant hypertag)
      (l/conde [(share-lex-entryo synt-constant sem-constant)
                (sem-sig sem-constant)
                (fs-assigne hypertag sem-term
                            {:head {:cat "n"}}
                            ,(rt sem-constant)
                            {:head {:cat "v"
                                    :trans "true"}}
                            ,(rt sem-constant))]
               [(fs-assigne hypertag sem-term
                            {:head {:cat "det"
                                    :det_type "indef"}}
                            ,(rt (ll [p q] (exists? (il [x] (and? (p x)
                                                                  (q x))))))
                            {:head {:cat "det"
                                    :det_type "def"}}
                            ,(rt (ll [p q] (forall? (il [x] (imp? (p x)
                                                                  (q x)))))))]))))

\end{verbatim}
  \caption{\label{fig:lex-impl} The implementation of the lexicons from
    subsection~\ref{ssec:acg}. NOTE: This is too verbose both to put on
    the paper and also to code. These examples demonstrates two common
    and related shapes of lexicons. The $\mathcal{L}$(\_{Syntax}) and
    the first half of the $\mathcal{L}$(\_{Sem}) are lexicalized, in the
    sense that the object term contains a lexical constant with the same
    lexical entry as the abstract constant.\footnote{This notion of
      ``lexicalized'' is quite different to the one I use in the rest of
      the thesis so far (which roughly means ``generated by a lexical
      database'') and also different from the one used by Kanazawa
      (FIX!).} This means that before constructing the object term, we
    have to acquire the object constant that is contained in the term
    and then we just fit it in of the recipes, which we choose according
    to the hypertag. The second half of $\mathcal{L}$(\_{Sem}) simply
    assigns object terms based on the hypertag. A more concise and
    desirable implementation would allow us simply write the pairs of
    hypertags and object terms (recipes encoded as lambdas). My plan is
    to have exactly this, but I have not thought yet about how to split
    the various aspects into different reusable abstractions.}
\end{figure}

\subsection{Testing signatures and lexicons}
