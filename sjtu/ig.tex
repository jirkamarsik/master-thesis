\section{Interaction Grammars}

We will briefly discuss one more grammar formalism and that is the
formalism of interaction grammars \cite{guillaume2009interaction}. This
formalism is of particular interest to us since it is based on the same
logical basis as ACGs and there is already a detailed wide-coverage
grammar written in it.

Interaction grammars are a grammatical formalism centered around the
concept of \emph{polarity}. An interaction grammar is a set of
(under-specified) tree descriptions which define a set of trees that can
be constructed by superposing some of these tree descriptions while
respecting the polarities described in the nodes of the individual tree
descriptions.

\subsection{The Mechanisms of Interaction Grammars}

We will illustrate the objects and mechanisms of interaction grammars on
a simple example sentence. Since the largest and most-developed
interaction grammar is the Frigram grammar of French, we will be using
French example sentences and their analyses according to
Frigram. Frigram will be discussed in more detail in
subsection~\ref{ssec:frigram}.

\begin{exe}
  \ex \label{ex:ig} Jean la voit.
\end{exe}

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{images/ig-parse.pdf}
  \caption{\label{fig:ig-parse} The syntactic tree of sentence
    (\ref{ex:ig}). The numbers at the top of the nodes are labels of
    nodes from the elementary polarized tree descriptions that generated
    the syntactic tree (see Figure \ref{fig:ig-eptds}).}
\end{figure}

Figure \ref{fig:ig-parse} shows the syntactic tree assigned by Frigram
to sentence (\ref{ex:ig}). It is a rooted ordered tree with the topmost
node being the root. Solid lines indicate immediate dominance with the
higher node being the parent and the lower node being the child. The
ordering of children is given by the arrows, which signify the immediate
precedence relation between sister nodes.

The nodes in the tree are of three different kinds with respect to the
phonological form of their subtree. We recognize the \emph{anchor
  nodes}, which are displayed in vivid yellow and which are leaf nodes
containing some non-empty string as their phonological content (written
in a gray rectangle at the top of the node). Then we have the
\emph{empty} nodes, which are drawn in white and whose phonological form
is empty. Finally, there are the pale yellow \emph{non-empty} nodes,
internal nodes whose phonological content (the yield of the subtree) is
not empty.

The nodes of the syntactic tree are also decorated with features.

Interaction grammars are a formalism enabling us to define sets of the
structures described above, the \emph{syntactic trees}. Similar to tree
adjoining grammars, interaction grammars are formulated in terms of a
set of elementary structures which can combine to produce the final
output structures. However, unlike in tree adjoining grammars, the
output structure is not constructed from the elementary structures via
some set of algebraic operations (substitution and adjunction in case of
TAGs). Instead, \emph{polarized tree descriptions} (PTDs), the
elementary structures of interaction grammars, impose constraints on the
final structure and a structure is said to be generated by some PTDs if
it is a minimal structure satisfying those constraints (we say it is a
\emph{minimal model}). This distinction separates TAGs as a formalism in
the generative-enumerative framework of syntax from IGs as a formalism
in the model-theoretic framework.

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{images/ig-eptds.pdf}
  \caption{\label{fig:ig-eptds} The elementary polarized tree
    descriptions used to generate the sentence (\ref{ex:ig}).}
\end{figure}

In Figure~\ref{fig:ig-eptds}, we can see the elementary polarized tree
descriptions (EPTDs) that generated the parse tree in
Figure~\ref{fig:ig-parse}.

As we said before, a PTD is a set of constraints on some syntactic
tree. Let us expound on what constraints the structures of
Figure~\ref{fig:ig-eptds} impose.

A node in a PTD can be read as a statement that there must exist a node
in the final syntactic tree that has compatible values for all the
features and that carries the same phonological string, if any. Such a
node of the syntactic tree is then called the \emph{interpretation} of
the PTD node (in Figure~\ref{fig:ig-parse}, every node of the syntactic
tree bears a list of the PTD nodes that it interprets, so you can see
exactly how the interpretation function works in our example).

A solid line between two nodes in a PTD means that the interpretations
of these two nodes must be in a parent-child relationship
(\emph{immediate dominance}). A dashed green arrow tells us that the
interpretations have to be sister nodes with the former preceding the
latter in the ordered tree (\emph{precedence}). Note that not all sister
nodes in a PTD have to be linked with this precedence relation, the tree
can be under-specified. See for example the nodes (1,1) and (1,5) in the
EPTD of the clitic pronoun \emph{la}.

The formalism also allows us to specify \emph{immediate precedence} and
(large) \emph{dominance}. The latter is useful for modelling unbounded
dependencies such as those between a relative pronoun and its trace in
some embedded clause of the relative clause, but it is not used in our
present example.

Finally, one kind of constraint that is used in our example is the
orange rectangle in node (3,0), which requires that the interpretation
of (3,0) is the rightmost daughter amongst its siblings.

Now that we have covered the structural constraints imposed by the tree
descriptions, we will turn our attention to the defining characteristic
of interaction grammars, the polarities. As you have noticed on
Figure~\ref{fig:ig-eptds}, some of the features in our PTDs are
annotated with special symbols and colors. These denote the different
polarities and are used by the formalism for two distinct purposes: the
positive ($\textcolor{red}{\rightarrow}$) and negative
($\textcolor{blue}{\leftarrow}$) polarities are used to model the
resource sensitivity of languages, while the virtual polarities
($\textcolor{purple}{\sim}$) are used for pattern matching against the
context.

The way the polarities are handled is that every model (output syntactic
tree) is required to have only saturated polarities on its features
(i.e. no positive, negative or virtual polarities). Whenever more than
one node of the PTD is interpreted by the same node of the syntactic
tree, the polarities of each feature are combined. The combination
mechanism allows us to combine a positively polarized feature with a
negatively polarized one to yield a saturated one. Virtual polarities
can only be eliminated by combining them with a saturated polarity (they
are analogous to the $=_{c}$ constraints of LFG
\cite{kaplan1982lexical}).

If we look at the PTDs of Figure~\ref{fig:ig-eptds}, we can see this
polarity mechanism in action. The EPTD of \emph{Jean} has a positive
$cat$ feature in its root node saying that it provides one $np$, and a
negative $funct$ feature saying that it expects some function. In the
EPTD of \emph{voit}, we have two nodes for the subject and the object,
both of them expecting an $np$ and providing them the $subj$ and $obj$
functions, respectively. The root of the EPTD then provides a complete
sentence of category $s$ and expects some function that this sentence
will play. The full stop EPTD finalizes the sentence by accepting a node
with category $s$ and giving it a $void$ function.

The clitic pronoun \emph{la} participates in the positive/negative
resource management system as well, since using the clitic fills up the
object slot in the valency of a verb. The EPTD of \emph{la} also uses
virtual features heavily to select the right place where to hang the
pronoun in the resulting tree.

Now that we understand the constraints imposed by PTDs, we can start to
see that the syntactic tree in Figure~\ref{fig:ig-parse} is truly a
model of the PTDs given in Figure~\ref{fig:ig-eptds} (furthermore, it is
the only minimal model). To illustrate more clearly how the polarities
and constraints of the individual PTDs end up generating the syntactic
tree of Figure~\ref{fig:ig-parse}, we can look at the result of
superposing the EPTDs of \emph{la} and \emph{voit} by merging the nodes
(1,2) and (1,5) with (2,2) and (2,4) respectively on
Figure~\ref{fig:ig-partial}.

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{images/ig-partial.pdf}
  \caption{\label{fig:ig-partial} The result of merging the EPTDs of
    \emph{la} and \emph{voit} by merging the nodes (1,2) and (1,5) with
    (2,2) and (2,4), respectively.}
\end{figure}

\subsection{Frigram, an Interaction Grammar}
\label{ssec:frigram}

The reason we are interested in interaction grammars is because of the
existence of Frigram, a large scale grammar of French which is
lexicalized by Frilex, an independent lexical resource that we will be
building our grammar on as well. Furthermore, as we will see
in~\ref{ssec:link-ig-acg}, interaction grammars are closely linked to
abstract categorial grammars, which makes Frigram a suitable grammar to
use as a guide when developing our grammar.

In this subsection, we will introduce the metagrammatical structure of
Frigram and talk about how it uses the formalism of interaction grammars
to solve several tricky linguistic phenomena.

\subsubsection{Frigram as a Metagrammar}

Frigram is a wide-coverage interaction grammar of French. As we
mentioned before, an interaction grammar is a set of elementary
polarized tree descriptions. In a lexicalized interaction grammar, each
of these EPTDs is connected to a specific wordform whose particular use
it describes. Given the amount of wordforms that a usable grammar of
French would need to cover, the number of EPTDs in our grammar could
easily reach hundreds of thousands, if not millions.

The first step in fighting this explosion is to factor out irrelevant
differences between similar wordforms. For this purpose, Frilex was
created. Frilex is a morphosyntactic lexicon of French compiled from
various other preexisting lexicons of French. It is in effect a large
relation linking the wordforms of French to \emph{hypertags}, feature
structures describing the morphological properties of the wordforms and
their syntactic valencies.

With Frilex in place, Frigram can be defined as a set of unanchored
EPTDs which are paired with feature structures that delimit the subset
of the Frilex items to which the EPTDs apply. This kind of simple
``metagrammar'' already saves us a lot of effort, the number of
unanchored EPTDs defined by Frigram is somewhere around
4000\footnote{\url{http://wikilligramme.loria.fr/doku.php?id=frigram:frigram}}.

However, defining some 4000 EPTDs manually still seems like a very
tedious and error prone process that is likely to lead to a grammar that
is hard to maintain. Many of the EPTDs have to repeatedly describe
common phenomena such as subject-predicate agreement or
predicate-argument saturation. Similarly as in other software endeavors,
it would be preferable to define these common patterns in some reusable
module and compose EPTDs from these building blocks. This is where XMG
steps in \cite{duchier2012metagrammars}.

XMG is a metagrammar compiler adding yet another level of indirection
between what the grammar author writes and what ends up in the
bottom-level (interaction) grammar. XMG provides a language that lets
the grammar author define the lexical items of his grammar and to
combine these definitions to yield new and more elaborate lexical items.

In the case of Frigram, the lexical items (termed \emph{classes} in XMG)
take the shape of PTDs coupled with feature structures which
circumscribe the interface to Frilex. Both tree descriptions and feature
structures have very natural ways of composing together, by conjunction
and unification, respectively. This, alongside with disjunction, are the
chief tools that the grammar authors can use (and in the case of
Frigram, have used) to succinctly define their grammar.

With the added capability of composing classes, the definition of
Frigram reduces to about 400 class definitions. Of these classes, 160
are terminal, meaning that they actually describe lexical items that are
to be included in the final grammar. Thanks to the disjunction
composition operator of XMG, these 160 classes end up generating the
4000 EPTDs.


\subsubsection{Linguistic Description in Frigram}

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{images/ig-neg.pdf}
  \caption{\label{fig:ig-neg} EPTDs of \emph{aucun} and \emph{ne}
    demonstrating the way polarities are used to model French negation.}
\end{figure}

We will briefly discuss how interaction grammars, and Frigram in
particular, handle a tricky linguistic phenomenon in an interesting way.
In French, negation is signalled by the particle \emph{ne}, which must
be accompanied by one of several designated determiners, pronouns or
adverbs. The particle \emph{ne} assumes a position right before the
inflected verb, but its partner, e.g. the determiner \emph{aucun} can
appear in a rich variety of positions:

\begin{exe}
  \ex \label{ex:aucun-suj} Aucun tatou ne court.
  \ex \label{ex:aucun-obj} Jean n'aime l'odeur d'aucun tatou.
  \ex \label{ex:aucun-bad} * Le tatou qu'aucun loup chasse ne court.
  \ex \label{ex:aucun-good} Le tatou qu'aucun loup ne chasse court.
\end{exe}

Sentences (\ref{ex:aucun-suj}) and (\ref{ex:aucun-obj}) demonstrate that
the \emph{aucun} determiner can be used both in the subject and object
positions, as a determiner of either one of the arguments directly or of
a noun phrase which complements one of them. However, it is not
admissible to place the determiner in an embedded clause as it is in
(\ref{ex:aucun-bad}). If the determiner \emph{aucun} is used in an
embedded clause, it is the embedded clause itself that must be negated,
as in sentence (\ref{ex:aucun-good}).

We can see the EPTDs of both \emph{aucun} and \emph{ne} on
Figure~\ref{fig:ig-neg}. This example demonstrates two points of
interest to us.

First, it uses a new polarized feature, $neg$, to model the fact that
the two wordforms which enable negation must always co-occur. The
particle \emph{ne} provides a positive $neg$ feature to the clause in
which it occurs and this positive polarity must be saturated by a
negative polarity contributed by one of the possible partner words, such
as \emph{aucun}. This shows that the usefulness of positive and negative
polarities extends beyond the $cat$/$funct$ pair that we have already
seen and which is used to model predicate/argument structure in a
similar way as was already done in tree adjoining grammars or categorial
grammars.

Second, the determiner \emph{aucun} can occur deep, e.g. inside a complement
of one of the arguments, but it cannot occur everywhere, as is
demonstrated by (\ref{ex:aucun-bad}), where we try to put the determiner
inside another embedded clause. There is a way to express this kind of
constraint in interaction grammars that we have not revealed
yet. Whenever we use a (large) dominance (a top-down green dashed line),
we usually want the range of nodes that this dominance relation can
cross to be somehow restricted. In interaction grammars, we can
associate a feature structure with a dominance constraint which will
force all of the nodes that span the distance between the two nodes
linked by the dominance constraint to unify with that feature structure.

In the notation used in our illustrations (which were generated by the
Leopar parser\footnote{\url{http://leopar.loria.fr/}}), this is conveyed
by putting all the restricting features as virtual features in a new
node and splicing the new node in between the two nodes linked by the
dominance constraint before (see the node (0,4) in
Figure~\ref{fig:ig-neg}, which merely serves to ensure that all the
nodes on the path from (0,3) to (0,2) have either $np$ or $pp$ as the
value of their $cat$ feature). When using this notation, the (large)
dominance constraint should then be read as not only stating that the
top node must be an ancestor to the bottom node, but also that any
intermediate nodes between the ancestor and successor must match the
ancestor's features.

\subsection{The Link with Abstract Categorial Grammars}
\label{ssec:link-ig-acg}

Both abstract categorial grammars and interaction grammars, having come
out of the same research team, are both closely related to linear
logic. Not only are these two formalisms based on the same logical
framework, there is a surprisingly direct connection between the
principles guiding the syntactic composition of constants having
intuitionistic implicative linear types in abstract categorial grammars
and of polarized tree descriptions in interaction grammars.

\subsubsection{Perrier's Theorem}

Perrier \cite{perrier1999intuitionistic} proved an interesting result
which bridges these two worlds. His theorem states that every IILL
(intuitionistic implicative linear logic) sequent $F_1, ..., F_n \vdash
G$ is provable if and only if the syntactic description $D((F_1 \limp
... \limp F_n \limp G)^+)$ is valid. The second premise deserves some
elaboration.

What we mean by a \emph{syntactic description} is something similar to a
polarized tree description, albeit more minimalistic. Syntactic
descriptions only talk about immediate dominance and (large) dominance,
there are no precedence constraints on sister nodes and (large)
dominance cannot be restricted by a feature structure. Nodes themselves
are no longer even represented by feature structures, but by simple
atomic categories paired with polarities. Every node in a syntactic
description is thus associated with a single atomic category and a
polarity which is either positive or negative.

A syntactic description that is \emph{valid} is one for which there
exists a model. A model of a syntactic description is defined in the
same manner as for polarized tree descriptions, but instead of unifying
feature structures, we simply demand that all nodes of the description
which map to a node in the model must have the same category and the
number of positively polarized nodes must equal the number of negatively
polarized nodes.

Finally, we explain the term $D((F_1 \limp ... \limp F_n \limp
G)^+)$. The $+$ operation recursively assigns positive and negative
polarities to all formula occurrences within $F_1 \limp ... \limp F_n
\limp G$. These polarities then drive the recursive definition of $D$,
which maps this polarized formula into a syntactic description. Since we
will not cover the theorem deeply in our treatise, we will settle for
knowing that the composition $D \circ +$ of the positive polarization
operation $+$ and the syntactic description producing operation $D$ maps
an IILL formula into a syntactic description such that the above-stated
theorem holds. Furthermore, this mapping can be easily constructed since
Perrier gives straightforward ways of computing the results of both
operations.

Perrier's theorem establishes a striking symmetry between the two
formalisms. Let us consider the case of parsing a sentence using a
lexicalized grammar in both formalisms. First, the sentence is tokenized
into wordforms and for each wordform, a lexical item is selected, be it
a typed constant in some abstract signature for ACGs or an EPTD for IGs.

In ACGs, we will try to take the typed constants and use them to produce
a term having some distinguished type $S$. Thanks to the Curry-Howard
correspondence, constructing a term having a given type is nothing more
than proving that term's type as a formula in the logic of our type
system. What this means is that our parsing problem boils down to
proving the IILL sequent $\tau_1, ..., \tau_n \vdash S$ (we omit the
inclusion of intuitionistic non-linear implications since they are not
relevant for parsing sentences). The sentence thus becomes parsable by
our grammar under the given lexical selection if and only if this
sequent is provable and furthermore, the proof of the sequent is our
desired syntactic structure in disguise.

Now in IGs, we will conjoin all the EPTDs into a single complex
polarized tree description. Parsing is then just a matter of finding a
model for this description. The sentence is therefore parsable by our
grammar under the given lexical selection if and only if this
description is valid and furthermore, the model which proves the
description valid is our desired syntactic structure.

This symmetry turns Perrier's theorem into a strong claim that lets us
transform the types of a signature into elementary syntactic
descriptions such that some selection of descriptions will generate some
tree if and only if the selection of the corresponding typed constants
generated some properly typed term. Inverting this transformation would
then let us take the elementary syntactic descriptions of an IG like
Frigram and turn them into types for an abstract signature of our ACG.

\subsubsection{Translating Tree Descriptions to Types}

However, just inverting $D \circ +$ mapping to obtain types from tree
descriptions is not sufficient. $D \circ +$, the transformation in
Perrier's theorem, is far from a bijection.

Firstly, it is not injective and several types end up being represented
by the same syntactic description. What this means is that the
transformation ignores the insignificant differences between formulas
such as $a \limp (b \limp c)$ and $b \limp (a \limp c)$. This would not
pose a serious issue though, as when we would try to invert $D \circ +$,
we could simply choose one of the possible formulas which fit the
syntactic description as our canonical representation since the
differences between the types would be inconsequential.

Secondly, a bigger problem is posed by the fact that $D \circ +$ is not
surjective. Perrier \cite{perrier2001intuitionistic} characterized the
set of syntactic descriptions that can be obtained as images of IILL
sequents using our transformation $D \circ +$, the set of \emph{IILL
  tree descriptions}, which is a proper subset of the set of all
syntactic descriptions. Specifically, in an IILL tree description, all
immediate dominance constraints go from negative nodes to positive
nodes, all (large) dominance constraints lead from positive nodes to
negative nodes and the root has a positive polarity.

While this property may not hold for the PTDs we find in Frigram, it
still gives us a general plan for recovering IILL types. The strategy
suggested by the description/formula connection given by the definition
of $D \circ +$ yields a mapping $F$ from the under-specified trees to
IILL formulas.

$$
F(N) = F(N_1) \limp ... \limp F(N_k) \limp cat(N)
$$

where $N_1, ..., N_k$ are the children of $N$ and $cat(N)$ is the
category of $N$. If $N$ is a leaf, then $F(N)$ is simply $cat(N)$. In
this definition, we treat the tree descriptions as trees with the union
of the immediate dominance and (large) dominance constraints serving as
the edges of the tree.

This strategy corresponds to our intuitions about how tree descriptions
should be expressed using IILL types. We will consider two very simple
cases to demonstrate it.

First, picture a simple tree description with a positive root $A$ and
two (necessarily) negative children $B$ and $C$. Our strategy would give
this tree the type $B \limp C \limp A$ (or $C \limp B \limp A$), which
correctly expresses the fact that this tree needs to consume some node
of category $B$ and another one of category $C$ and is able to provide a
node of category $A$. This tree description can also be easily
translated to TAGs by turning the negative leaf nodes into substitution
nodes. The conventional translation of TAGs into categorial grammars
then confirms the type proposed by our strategy.

The second example will demonstrate the translation of a Frigram
EPTD. Before we do that, however, we will first discuss some of the ways
the actual PTDs of interaction grammars and Frigram differ from the IILL
tree descriptions we just talked about.

\subsubsection{IG Features Outside the Scope of Perrier's Theorem}

The PTDs of IGs include precedence constraints. We can see them as
serving two purposes, to both provide an ordering of the constituents
and to further constrain syntactic composition. In our translation to an
ACG, we could imagine handling the first role, that of providing word
order, by defining a lexicon from our abstract signature. Each constant
would combine its arguments in the order in which their corresponding
subtrees appeared in the ordered PTD. However, the second role of
precedence constraints, to establish further restrictions on syntactic
composition, would be lost.

Another large difference is the use of multiple independent polarized
features per node. A possible solution here is to salvage the most
salient polarized feature, $cat$ (possibly with its complementary
feature, $funct$), and ignore the others. The results and intuitions we
have about interpreting tree descriptions as types do not offer an easy
way of modeling polarities across different dimensions, features, at the
same time. However, dropping the other polarized features would cost us
the treatments of some phenomena, such as the paired grammatical words
for negation in French, which were handled using the $neg$ feature.

Next up are the virtual features. They serve multiple purposes as
well. Most often they are used to select nodes in the context and to
link them to the nodes contributed by the EPTD.

A common pattern using virtual features is Frigram's handling of
modifiers. The way we would model a modifier in categorial grammars
would be to have a function of type $X \limp X$ where $X$ is the type of
the constituent being modified. In TAGs, we would have an adjunction
tree with a root and a foot node of category $X$ and which pins the
modifier on the $X$ phrase. This approach would work in interaction
grammars as well but the formalism offers other solutions too. The
designers of Frigram have opted for an alternate treatment in which
modifying a constituent does not increase the depth of the parse
tree. Modifiers are adjoined as sister nodes of the constituents they
modify and it is through virtual features that IGs can ensure that a
modifier is inserted only in the proper context.

See Figure~\ref{fig:ig-adj} for an example of a modifier adjective in
Frigram. Nodes (1,3) and (1,2) select the noun to be modified and its
projection, respectively. Singling out this pair of nodes lets us hang
the adjective as a child of the projection and state that it must
precede the noun in the word order. We can see that this does not
increase the depth of the noun in the parse tree, since the noun is only
referred to using virtual features in this EPTD, meaning that some other
EPTD must provide the nodes that will saturate these virtual features.

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{images/ig-adj.pdf}
  \caption{\label{fig:ig-adj} An EPTD for the adjective \emph{grand} in
    Frigram.}
\end{figure}

Understanding this pattern can give us a useful tool for reading
EPTDs. Consider the EPTD of \emph{la} we saw in
Figure~\ref{fig:ig-eptds}. The EPTD can now be seen as applying this
pattern twice at the same time to both cliticize the pronoun to the
verbal kernel and to fill the object valency slot of the verb.

Another use of virtual features in IGs is to restrict the range of
(large) dominance relations. We have seen this before in the EPTD of the
determiner \emph{aucun} (Figure~\ref{fig:ig-neg}), which had to reach up
to the nearest containing clause and hang a negative $neg$ feature
there.  It is also present in the EPTD of the relative pronoun $que$
(Figure~\ref{fig:ig-que}) which will bring us back to the translation
strategy proposed above.

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{images/ig-que.pdf}
  \caption{\label{fig:ig-que} An EPTD of the relative pronoun $que$.}
\end{figure}

Once again, at the root of the EPTD we see the modifier pattern in
action. If we take the subtree formed by the nodes (2,8) and (2,9) to
represent a modification of some constituent, say a noun, then we get a
type like $n \limp n$. However, the EPTD also contains positive and
negative $cat$ polarities. The node (2,0) has a negative $cat$ polarity
and we can consider it as an argument, meaning that the type we would
like to derive would look like $F((2,0)) \limp (n \limp n)$. However,
our strategy $F$ has nothing to say about virtual features or (large)
dominance constraints, so we will have to ignore those. If we look only
at the positive and negative $cat$ features, $F((2,0))$ becomes $np
\limp s$, which would give us $(np \limp s) \limp (n \limp n)$ as the
final type.

However, this translation is very lossy in terms of its discriminatory
power. Many of the constraints which were enforced by the original EPTD
are now ignored. We can imagine improving our strategy by using the
feature structures as our types instead of atomic categories, which
would not only ensure that e.g. the $sent\_type$ of the embedded clause
must be $decl$, but would also capture the information in the polarized
$funct$ feature so that the relative pronoun \emph{que} is not permitted
to extract subjects. There are still constraints that we would miss out
on though, such as the fact that the subject (2,3) of the clause (2,0)
must precede any embedded clauses (2,6)...(2,5), the fact that any such
embedded clauses must serve either the $modal$ or the $obj$ functions
and the fact that the embedded clause (2,5) must have a subject (2,4).

Other constraints that we did not mention also do not have a direct
equivalent in ACGs. One example would be a feature of IGs that we have
not even mentioned which allows the grammar author to provide an
\emph{exhaustive} list of children of a node in a PTD.

Another kind of tricky constraint are the orange rectangles that we can
see in Figure~\ref{fig:ig-que} which enforce that a constituent must be
the leftmost or rightmost daughter of its parent. The problem with these
is similar as with precedence relations. To model their impact on word
order would be easy using an ACG lexicon, but to model their effect on
restricting syntactic composition would be more tricky.

In the end, even though both formalisms (IGs and ACGs) are built on the
framework of linear logic and their fundamental principles of syntactic
composition have been linked together by Perrier's theorem, we
conjecture that straightforwardly embedding Frigram into an ACG or
constructing some automated technique for directly translating the
Frigram EPTDs into ACG types and terms would not be feasible. We suspect
that the incompatibility of these two formalisms is due to ACGs being a
formalism based in the generative-enumerative framework of syntax while
IGs are based in the model-theoretic approach.

In our grammar, we will still gladly use the lexicon Frilex though,
which proves the usefulness of clearly defining the interface between
the lexicon and the grammar in the architecture of Frigram. In our
grammar, Frigram can still serve as a guide on how to interface with
Frilex, on which phenomena need to be covered and in what detail, and
what is a good way to structure the reusable components of a grammar of
French.
