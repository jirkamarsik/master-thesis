\chapter{Implementing Wide-Coverage Abstract Categorial Grammars}
\chaptermark{Implementing Wide-Coverage ACGs}
\label{chap:implementation}

Our goal is to develop large abstract categorial grammars which cover a
variety of linguistic phenomena. All of these phenomena have been
studied in detail but we would like to have a grammar which gracefully
combines all these solutions. At this scale, working out the grammar on
paper stops being sufficient to detect all the subtle interactions and
it is desirable to have a formalized representation that can be reasoned
about or tested by a computer.

To some extent, this need is already met by the \textbf{ACG development
  toolkit}\footnote{\url{http://www.loria.fr/equipes/calligramme/acg/}}. The
toolkit reads in signature and lexicon definitions and offers two modes
of operation. In the first, it checks the validity of the defined
ACG. That is to say, it checks whether the terms assigned by the
lexicons are well-typed and whether their types are consistent with the
object types dictated by the lexicon. The second mode of operation
offers an interactive experience in which the user is free to run type
inference on terms built on one of their signatures to find out whether
they are well-typed and if so, what is their principal type. The
interactive mode then lets the user apply a lexicon, or a composition of
lexicons, to a term and get the $\beta$-normal form of the object term.

We consider the above capabilities vital for doing any involved grammar
engineering in the framework of ACGs. However, there are some
limitations which make it not sufficient for our needs. The foremost of
these is that the signatures and lexicons are all explicitly realized in
(primary) memory and the toolkit expects them to be given directly in a
file. The grammars that we would like to develop will cover an
exhaustive list of wordforms and the size of our grammar definitions
would therefore grow proportionately with the size of our dictionary.

Given this setup, there are several approaches we might try, none of
them too appealing. We might write our metagrammar in a different format
and use a tool which would combine the metagrammar and the lexical
database\footnote{We will be using the term \emph{lexical database} to
  refer to what is usually called a lexicon, in order not to cause
  confusion with the lexicons of ACGs.} to produce the
lexicalized\footnote{We will be using the term \emph{lexicalized} in the
  context of ACGs to mean that the elements of the grammar (the typed
  constants and their images given by lexicons) are generated by a
  lexical database. This is not to be confused with the term used in
  \cite{yoshinaka2005complexity}.} ACG in a direct representation, ready
to be loaded by the toolkit. This workflow would make development on the
grammar tedious as every time we would change the metagrammar, we would
have to regenerate the direct representation and then load it in the ACG
toolkit. With extra effort, the former cost could be alleviated by
devising a system for regenerating only parts of the grammars which will
have changed since last time. However, we would still have to pay the
price of loading the entire grammar into the toolkit before being able
to interact with it again.

Furthermore, we could pick a small representative fragment of the
dictionary and test only on that fragment during the development of our
grammar, to reduce the time it takes to both generate the direct
representation of the grammar and the time it takes to load it into the
toolkit. This would have made developing the grammar already tenable.

However, in our approach, we have opted to spend more time on tool
development to ease subsequent work and we have developed a system which
makes defining and experimenting with lexicalized ACGs easier. The
metagrammars are defined as relational procedures which link the items
of the lexical database to elements of signatures and lexicons. The
grammars can be redefined without having to reload the lexical database
or any file of similar scale. Type inference and lexicon application
with $\beta$-normalization are provided as relations which can be used
interactively or as part of larger programs. En example of such a larger
program might be a routine to check the validity of an ACG or to test
some of its properties.

In this chapter, we will spend some time explaining the design rationale
underlying the system and the form it has now. The source code of the
system in its current state can be perused at
\url{https://github.com/jirkamarsik/acg-clj}.

\section{The Tools and Techniques}

The dominant decision which we made during the development of the system
was to use \emph{relational programming}. Relational programming is a
discipline of logic programming where relations are written in a
``pure'' style \cite{byrd2010relational}. This means that relations
cannot be picky about which arguments are passed in as inputs and which
as outputs, all modes must be permissible. Accepting such a discipline
prohibits us from using some of the special operators of Prolog such as
cut (\texttt{!}) and \texttt{copy\_term/2}. Instead, we must resort to
other techniques which do not break purity, like \emph{constraint logic
  programming}, which allows us to express properties outside of the
scope of unification by attaching constraints to terms and verifying
them at the proper time.

By insisting on purity, I/O becomes more difficult as we would not allow
the side-effectful pseudo-relations of Prolog. Therefore, we will not
force ourselves to implement the entire program front-to-back using only
relational programming. Instead, we will use a relational programming
system, miniKanren, which is embedded inside a functional programming
language. This lets us reap the benefits of relational programming
inside the core logic of our program while deferring to the functional
programming language to provide I/O and other bookkeeping operations.

\subsection{The Case for Relational Programming}

There are several reasons why we might want to choose a relational
programming system for our implementation. First off, the problems which
constitute the algorithmic core of our program are often mere textbook
examples of how to use a relational programming system. This goes for
type inference/checking/inhabitation, which are all implemented by a
single relation which is often used as a kind of ``Hello World'' program
in the world of relational programming. Similarly, implementing
$\beta$-reduction cleanly (without having to handle variable clashes by
generating new variables and $\alpha$-converting) is a motivational
example for introducing \emph{nominal logic programming}. Finally, to
reach feature parity with the ACG toolkit, one would only need to
implement the mapping of a lexicon over a term, which is a simple
functor.

The use of logic programming techniques is also very popular in our
domain. If we look at the system demonstrations at the \emph{Logical
  Aspects of Computational Linguistics 2012} conference, we see that 3
out of the 5 proposals have been implemented in a declarative/logic
programming system (Oz, Prolog). 2 of the 3 systems are parsers for
categorial grammars (automated theorem provers), which might turn out to
be an interesting direction for our system to go into as well, as
practical applications appear on the horizon.\footnote{Theorem provers
  are another problem area where miniKanren fits well, see
  $\alpha$leanTAP \cite{near2008alpha}.}

The third system at the conference was XMG, a language for writing
metagrammars, which is another goal of our system, one which
distinguishes it from the original ACG toolkit. We believe that using a
declarative programming system such as miniKanren with the ability to
extend it using a practical functional programming language presents an
empowering way of defining metagrammars. This allows the grammar
designer to compose her grammar in any way she sees fit and to use the
metaprogramming facilities of the underlying programming language to
transcribe the grammar in a way that makes the most sense in her case.

Finally, some of the structures that we will encounter in our domain are
more directly modelled in mathematics as relations rather than
functions. In a relational programming system, to implement a relation,
one just writes a relation as it is, whereas if we were forced to encode
everything in functions, we would have to simulate relational behavior
manually. Frilex, our lexical database, is an example of a structure
that has this relational shape since a single wordform can be mapped to
multiple hypertags. We will see another example of a relation within our
domain in \ref{ssec:def-sig} and we will also discuss the advantages and
drawbacks of modelling functions using relations.

\subsection{Choice of Implementation Language}

There are two programming languages which play host to a substantial
implementation of miniKanren (meaning one that has extensible
unification and/or constraints and that includes support for nominal
logic), Racket\footnote{\url{https://github.com/calvis/cKanren}} and
Clojure\footnote{\url{https://github.com/clojure/core.logic}}. The two
languages are very similar to each other and therefore the choice is of
no great significance. An appealing feature of Clojure is that it uses
abstract collection types pervasively instead of coupling the standard
operations to specific data structures such as lists and cons cells and
that it follows other similarly enlightened design decisions. Racket, on
the other hand, has had more time to mature, has better metaprogramming
facilities and is more recognized in academia. In developing our system,
we have settled for Clojure. One of the advantages of doing so is that
the Clojure implementation of miniKanren, dubbed core.logic, has
inspired the software community and the project has attracted
contributors that submit numerous issue reports and
patches.\footnote{\url{http://dev.clojure.org/jira/browse/LOGIC}}


\section{Defining Signatures and Lexicons}

In this section, we will go over the representation of signatures and
lexicons in our implementation and the basic facilities for defining
them.

\subsection{Defining Signatures}
\label{ssec:def-sig}

In~\ref{ssec:sig}, we formally presented signatures as triples
$\mathopen{<}A, C, \tau\mathclose{>}$. However, the set $C$ can be
always reconstructed as the domain of $\tau$ and the set $A$ is just the
set of all the atomic types occurring in the range of $\tau$. Therefore,
to define the signature $\mathopen{<}A, C, \tau\mathclose{>}$, it is
enough to provide the function $\tau$, which in its set-theoretical
realization is just a set of pairs. In relational programming, a
specific set can be easily encoded as a unary relation so a unary
relation will serve as our representation of a signature and defining a
signature will be a matter of constructing this relation.

\subsubsection{The Relationship Between the Lexical Database and a Signature}

We have our lexical database, a set of pairs of wordforms and hypertags,
and we want to arrive at a set of pairs of constants and their
types. This mapping can be decomposed into smaller parts by considering
the contribution of every part of the lexical database
individually. What will be the right model for the link between some
part of the lexical database and some part of our signature?

\begin{description}
  \item[Function of wordform] We could imagine that every different
    wordform of the lexical database will end up being mapped to some
    constant in the signature. However, this is unsuitable since the
    lexical database is a relation that can assign more than one
    distinct hypertag to a single wordform (e.g. a wordform with
    different possible part of speech) and we would like to have
    constants of different types for each of these hypertags.
  \item[Function of hypertag] Conversely, we might imagine that the
    hypertags of the lexical database are mapped via some function to
    the constants of the signature. Here we run into trouble when we
    consider variations in spelling or phonology. We might have two
    items in our lexical database that share the same hypertag but whose
    wordforms still differ. We would then like to have two distinct
    constants in our signature as well, so that when these two constants
    are mapped by the surface lexicon, they will yield two different
    strings.
  \item[Function of lexical entry] Since neither the wordform nor the
    hypertag is enough, we could consider the entire lexical entry (an
    element of the lexical database, a pair of a wordform and one of its
    hypertags). This solves both of the above problems but it is still
    not perfect. Consider the case of a determiner and some semantic
    signature. Since our link between elements of the lexical database
    and of our signature is a function, there must be a constant in the
    signature for every item of our lexical database. If you recall
    in~\ref{ssec:acg-sem}, the constants of the semantic signature
    $\Sigma_{Sem}$ contained predicates for the common nouns \emph{man}
    and \emph{woman} and for the verb \emph{loves} and some common
    logical furniture. However, it did not contain any constants which
    would correspond to the determiners \emph{every} and \emph{some} and
    therefore there would be no constant that the function could assign
    to these two wordforms.
  \item[Partial function of lexical entry] We could fix the issue above
    by using a partial function. This accounts for cases when a lexical
    entry has no constants to represent it in a signature. Nevertheless,
    this only fixes half of the problem. We will consider the single
    lexical entry which represents e.g. the 3rd person singular present
    tense form \emph{loves} of the transitive verb \emph{love}. A
    legitimate account of subject/object scope ambiguity would be to
    consider two constants in our abstract syntactic signature that have
    distinct images in some semantic lexicon, one giving the outer scope
    to the subject and the other to the object. This means that in order
    to define such a signature, we would need to link a single lexical
    entry in our database to more than one constant in the signature.
  \item[Relation between lexical entry and constant] Finally,
    considering all the above scenarios, the only mathematical model
    general enough to handle all of these cases is a relation. In our
    system, this will correspond exactly to a specific relation that
    will be provided by the grammar designer.
\end{description}

\subsubsection{Representing Constants and Defining Signatures}

We have established that signatures will be defined by a relation, a
relation between a lexical entry (a wordform and a hypertag) and a
constant (an identity\footnote{The identity of a constant is simply the
  thing that distinguishes it from other constants of the same signature
  having the same type.} and a type). The identity of a lexical constant
also has a complex structure. We know that the lexical constants we
define using the relation described above are always linked to a lexical
entry. This lexical entry at least partially identifies the
constant. For cases where we have more than one constant per lexical
entry, such as in the example of the two different scope readings of
\emph{loves}, we need some extra piece of information to specify exactly
which of the two constants we are referring to (we call this extra piece
of information a \emph{specifier}). This specifier can have any shape
that the grammar designer desires. We can see an example of a lexical
constant in Figure~\ref{fig:lex-const}.

\begin{figure}[h]
  \centering
\begin{verbatim}
{:type [-> NP [-> NP S]]
 :id {:lex-entry {:wordform "loves"
                  :hypertag ...}
      :spec {:scope :subject-wide}}}
\end{verbatim}
  \caption{\label{fig:lex-const} An example of a lexical constant from a
    hypothetical syntactic signature (the hypertag is elided for brevity).}
\end{figure}

The system takes care of wiring up the relations and assembling the
signature that yields these structures. The grammar designer's duty is
to provide a 4-ary relation that links the four parts of a lexical
constant: the wordform, the hypertag, the specifier and the type. This
relation can be thought of as a multi-valued ``function'' that receives
as input a wordform and a hypertag (that is, a single lexical entry) and
outputs (possibly multiple) pairs of specifiers and types.

Since we are working in a functional programming language that supports
manipulation of higher-order functions, we can provide helper functions
which construct the signature relations for the grammar designer in some
simple cases. Our library thus provides functions like
\texttt{unitypedr}, which expects a type and returns a signature that
assigns to every lexical item a single constant whose specifier is $nil$
and whose type is the supplied type, or \texttt{ht->typer}, which
expects a mapping from hypertag patterns to types and returns a
signature which tries to map the hypertags of lexical items against the
patterns and yields constants with the corresponding types and $nil$
specifiers. When other patterns emerge, the grammar designer is
completely free to implement her new abstractions using the full power
of the functional programming language.

See Figure~\ref{fig:lex-sig-impl} for an example of implementing a
lexicalized version of the $\Sigma_{Synt}$ signature
from~\ref{ssec:example-sig} using the \texttt{ht->typer} abstraction.

\begin{figure}
  \centering
\begin{verbatim}
(ht->typer {{:head {:cat "n"}}      'N
            {:head {:cat "v"
                    :trans "true"}} (-> 'NP 'NP 'S)
            {:head {:cat "det"}}    (-> 'N (-> 'NP 'S) 'S)})
\end{verbatim}
  \caption{\label{fig:lex-sig-impl} A lexicalized version of the
    syntactic signature from~\ref{ssec:example-sig}.}
\end{figure}

In the above paragraphs, we have been referring to the constants that we
were defining as \emph{lexical constants}. That was to distinguish them
from the \emph{non-lexical constants} that we will introduce now. The
reason for having non-lexical constants is that not every constant of
every signature is linked to (generated by) some item of our lexical
database. Consider, for example, the empty string $\epsilon$ and the
string concatenation operator $+$ of the $\Sigma_{String}$ signature we
gave in~\ref{ssec:example-sig} or the quantifiers and logical
connectives of the $\Sigma_{Sem}$ signature in~\ref{ssec:acg-sem}.
These constants are not associated to any lexical entry and therefore
their representations must be different. See
Figure~\ref{fig:nonlex-const} for an example of the representation we
use for non-lexicalized constants.

\begin{figure}
  \centering
\begin{verbatim}
{:type [-> T [-> T T]]
 :id {:constant-name and?}}
\end{verbatim}
  \caption{\label{fig:nonlex-const} An example of the representation of
    a non-lexicalized constant (in this case, it is the conjunction
    operator in a semantic signature).}
\end{figure}

Signatures of non-lexical constants can be simply defined by a mapping
from the constant names to their types as in
Figure~\ref{fig:nonlex-sig-impl}. Such signatures will generally not be
sufficient by themselves and will need to be complemented by other
signatures which contain lexical constants. Since we represent
signatures as unary relations, combining them to produce their unions or
intersections is just a matter of taking their disjunctions
(\texttt{ors}) or conjunctions (\texttt{ands}), respectively.

\begin{figure}
  \centering
\begin{verbatim}
(nonlex-sigr {'and?    (-> 'T 'T 'T)
              'imp?    (-> 'T 'T 'T)
              'forall? (-> (=> 'E 'T) 'T)
              'exists? (-> (=> 'E 'T) 'T)})
\end{verbatim}
  \caption{\label{fig:nonlex-sig-impl} A signature of the non-lexical
    semantic constants belonging to the semantic signature
    of~\ref{ssec:acg-sem}. The single arrow \texttt{->} corresponds to
    linear implication while the double arrow \texttt{=>} corresponds to
    intuitionistic implication.}
\end{figure}

With this, we have enough tools to fully define all the signatures of
section~\ref{sec:acg} in a lexicalized manner, see
Figure~\ref{fig:example-sig-impl}.

\begin{figure}
  \centering
\begin{verbatim}
(def synt-sig
  (ht->typer {{:head {:cat "n"}}      'N
              {:head {:cat "v"
                      :trans "true"}} (-> 'NP 'NP 'S)
              {:head {:cat "det"}}    (-> 'N (-> 'NP 'S) 'S)}))

(def string-sig
  (ors (nonlex-sigr {'++ (-> 'Str 'Str 'Str)
                     'empty-str 'Str})
       (unitypedr 'Str)))

(def sem-sig
  (ors (nonlex-sigr {'and?    (-> 'T 'T 'T)
                     'imp?    (-> 'T 'T 'T)
                     'forall? (-> (=> 'E 'T) 'T)
                     'exists? (-> (=> 'E 'T) 'T)})
       (ht->typer {{:head {:cat "n"}}      (-> 'E 'T)
                   {:head {:cat "v"
                           :trans "true"}} (-> 'E 'E 'T)})))
\end{verbatim}
  \caption{\label{fig:example-sig-impl} The definitions of the
    lexicalized versions of the example signatures of
    section~\ref{sec:acg}.}
\end{figure}


\subsection{Defining Lexicons}
\label{ssec:def-lex}

We have explained how we can use elements of our lexical database to
define signatures. Now, we will turn to lexicons. Similar to how we
reduced signatures from the formal triples $\mathopen{<}A, C,
\tau\mathclose{>}$ to just the type assignment function $\tau$, we can
reduce lexicons from the pairs $\mathopen{<}F, G\mathclose{>}$ to just
$G$, the part which operates on terms. $F$, or at least its relevant
subset, can be easily retrieved from $G$. We take the pairs of abstract
constants and object terms that $G$ is composed of, we take their types
and we arrive at pairs of abstract-level and object-level types. These
pairs form a subset of $\hat{F}$. $F$ (or at least its relevant subset),
which only maps the atomic abstract types, can be inferred from this
subset of $\hat{F}$.

In the relational framework, a mapping can be represented most directly
as a binary relation and that is the representation of lexicons that we
use in our system. In the previous subsection, we talked a lot about how
individual entries in the lexical database generate the items of a
signature. Solving the problem for signatures also solved it for
lexicons, since a lexicon contains exactly one object term for every
constant in its abstract signature. Therefore, a pair of an abstract
constant and an object term belonging to some lexicon is generated by
the same lexical entry that generated the abstract constant and its
type.

Our representation of lexical constants was purposefully engineered so
as to make the lexical entry directly accessible to the lexicon
implementation. Since the lexicon is implemented by the grammar designer
as a binary relation between constants and terms, the grammar designer
can inspect the abstract constant and directly access the hypertag and
wordform of the lexical entry that generated it, the specifier that was
assigned to it and the type of the constant, and then express the object
term assigned to the constant using any combination of these.

\subsubsection{Succinct Definitions of Lexicons}

The above is already enough to define any lexicon. However, our toolkit
presents convenience facilities that capture common forms of
lexicons. With these, we will be able to define lexicalized versions of
the lexicons of section~\ref{sec:acg} in a concise point-free
style. Furthermore, the grammar designer is not limited to the
convenience facilities we have provided and is completely free to use
the same programming language we have used to build her own abstractions
on top of ours right within her grammar definition.

\begin{figure}
  \centering
\begin{verbatim}
(def syntax-lexo
  (with-sig-consts string-sig
    (lexicalizer string-sig
                 (ht-lexiconr {{:head {:cat "n"}}
                               ,(rt (ll [_] _))
                               {:head {:cat "v"
                                       :trans "true"}}
                               ,(rt (ll [_ x y] (++ (++ x _) y)))
                               {:head {:cat "det"}}
                               ,(rt (ll [_ x R] (R (++ _ x))))}))))

(def sem-lexo
  (with-sig-consts sem-sig
    (orr (lexicalizer sem-sig
                      (ht-lexiconr {{:head {:cat "n"}}
                                    ,(rt (ll [_ x] (_ x)))
                                    {:head {:cat "v"
                                            :trans "true"}}
                                    ,(rt (ll [_ s o] (_ s o)))}))
         (ht-lexiconr {{:head {:cat "det"
                               :lemma "un"}}
                       ,(rt (ll [P Q] (exists? (il [x] (and? (P x)
                                                             (Q x))))))
                       {:head {:cat "det"
                               :lemma "chaque"}}
                       ,(rt (ll [P Q] (forall? (il [x] (imp? (P x)
                                                             (Q x))))))}))))
\end{verbatim}
  \caption{\label{fig:lex-impl} The definitions of the lexicalized
    versions of the lexicons of section~\ref{sec:acg}.}
\end{figure}

We have the lexicon analogues to \texttt{nonlex-sigr} and
\texttt{ht->typer}, they are called \texttt{nonlex-lexiconr} and
\texttt{ht-lexiconr} and they work the same way as the signature
versions but assign object terms instead of types.

When mapping lexical constants by a lexicon, we usually produce object
terms which contain constants of the object signature that are generated
by the same lexical entry as the abstract constant
(e.g. $\mathcal{L}_{syntax}(\synt{love}) = \lambda^{\circ} x y.\ x +
loves + y$). Managing this involves some tedious boilerplate that can be
avoided by another convenience facility. First, we define a lexicon that
produces the desired object term but with the object constant abstracted
out (e.g. $\lambda^{\circ} c x y.\ x + c + y$). Then, we can apply the
\texttt{lexicalizer} function to the object signature and to this
lexicon to get the desired lexicon which fills in the correct object
constants.

It is also useful to be able to easily insert the non-lexical constants
of the object signature into the object terms we are giving when
defining a lexicon. This service is provided by the anaphoric macro
\texttt{with-sig-consts} which introduces all of the non-lexical
constants of a signature into scope for easy use inside of lexicon
definitions.

Finally, since lexicons are again just relations, we can take their
unions and intersections using disjunction (\texttt{orr}) and
conjunction (\texttt{andr}), respectively.

This gives us everything we need to define the lexicalized versions of
the lexicons of section~\ref{sec:acg} in a concise and elegant
manner. See Figure~\ref{fig:lex-impl} for the definitions.


\section{Checking Signatures and Lexicons}

Since we let the grammar designer define signatures and lexicons using
arbitrary relations, she can also end up defining relations which cannot
be interpreted as well-formed signatures or lexicons. For this reason,
our toolkit provides a set of testing facilities which help check that
the signatures and lexicons are well-defined.

\subsection{Checking Properties of Large Structures}

As opposed to the existing \textbf{ACG development toolkit}, we cannot
easily enumerate all of the elements in a given signature or lexicon and
check that they are all consistent. The size of the lexicalized grammar
is too large to make this approach practical.

We solve this problem by demanding that the grammar designer names some
subset of the structure and testing is done only on that subset. In our
case, the most practical solution has been to let the designer just list
wordforms that generate a sufficient subset of the structure, i.e. the
designer lists one wordform per every category she has handled in her
grammar.

A more comfortable solution, which would also be more technically
involved, would be to have the system test the grammar on an
automatically deduced selection of constants such that they are
guaranteed to cover all the cases and code paths in the grammar
designer's definitions. Implementing such a system could still be
manageable given the small number of primitives the relational
programming system is built upon.

\subsection{Checking Signatures}
\label{ssec:check-sig}

The crucial property that we need to check for in signatures is that the
relation between the identities of constants and their types is a
function. This simply reduces to querying the system for the number of
typed constants belonging to the signature and having a specific
identity. If this number is one for all the identities in our test set,
we have verified that (the tested subset of) the signature assigns
exactly one type to every one of its constants.

\subsection{Checking Lexicons}

For lexicons, we check the three following properties:

\begin{description}
\item[The lexicon is a function,] assigning exactly one object term to
  each abstract constant. The verification proceeds analogously to the
  case of checking signatures (subsection \ref{ssec:check-sig}).

\item[The lexicon assigns only well-typed object terms.] This is a
  consequence of the homomorphism property of lexicons. We verify it by
  checking whether we can infer some type for every object term assigned
  to a constant in the test set.

\item[The lexicon has the homomorphism property.] We use the process
  that we described in subsection~\ref{ssec:def-lex} for inferring the
  type mapping of a lexicon from its term mapping.

  We first enumerate the pairs of abstract constants belonging to the
  test set and the object terms assigned to them by the lexicon. We then
  take the types of these pairs. Finally, we state, by using
  unification, that this set of pairs is a subset of the homomorphic
  extension of some mapping of atomic abstract types and let the
  relational programming system find us that mapping. If it succeeds, we
  have verified the homomorphism property of the lexicon.
\end{description}

The above technique of verifying the signature and lexicon definitions
by automated tests is not only useful for checking their
well-formedness. The grammar designer can go further and implement new
tests which demonstrate grammar-specific properties, an example of which
would be the $cat$-$funct$ principle in Frigram that imposes constraints
on the relationship between the $cat$ and $funct$ features in the EPTDs
of the grammar.
