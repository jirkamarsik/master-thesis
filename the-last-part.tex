\section{The Last Part}

In this section, we will attempt to incorporate accounts of several
linguistic phenomena in a single framework, highlight the modularity
issues this brings up and propose some solutions to them.


\subsection{Negation}

We will start with an account of the paired grammatical words mechanism
for negation in French. Recall the EPTDs we have seen
in~\ref{sssec:frigram}, repeated here on Figure~\ref{fig:ig-neg-rep}.

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{ig-neg.pdf}
  \caption{\label{fig:ig-neg-rep} Frigram EPTDs for a negative
    determiner and the paired grammatical word $ne$.}
\end{figure}

At a basic level, $aucun$ functions like any other determiner and $ne$
as any verb modifier. However, we want to encode the constraint that
whenever a verb is modified by $ne$, the modified verb will demand that
one of its arguments contains a negative determiner. Since we need to
make a distinction between phrases that contain negative determiners and
those that do not, we will need to provide different types for both (two
terms with the same type are indistinguishable w.r.t. syntactic
composition in a type-logical grammar such as ACGs). Our type for
$aucun$ will thus end up being $N\_NEG{=}F \limp ((NP\_NEG{=}T \limp S)
\limp S)$ instead of the usual $N \limp ((NP \limp S) \limp
S)$.\footnote{Strictly following the EPTD for $aucun$ would lead us to
  give it a type $DET\_NEG{=}F$ and then give the two types
  $DET\_NEG{=}F \limp ((NP\_NEG{=}F \limp S) \limp S)$ and $DET\_NEG{=}T
  \limp ((NP\_NEG{=}T \limp S) \limp S)$ to nouns. In this
  demonstration, we prefer to align our examples with the treatment
  which is customary to ACG research.} The type for $le$ will need to be
able handle cases where its argument noun either already contains a
negative determiner or not.

$$
N_{aucun} : N\_NEG{=}F \limp ((NP\_NEG{=}T \limp S) \limp S)
$$
$$
N_{le_1} : N\_NEG{=}F \limp ((NP\_NEG{=}F \limp S) \limp S)
$$
$$
N_{le_2} : N\_NEG{=}T \limp ((NP\_NEG{=}T \limp S) \limp S)
$$


The type of $ne$ will be more verbose, as it will consume a verb and
transform its valency so that it demands that exactly one of its $NP$
arguments contains a negative determiner.

$$
N_{ne_{tv_1}} : (NP\_NEG{=}F \limp NP\_NEG{=}F \limp S) \limp (NP\_NEG{=}T \limp NP\_NEG{=}F \limp S)
$$
$$
N_{ne_{tv_2}} : (NP\_NEG{=}F \limp NP\_NEG{=}F \limp S) \limp (NP\_NEG{=}F \limp NP\_NEG{=}T \limp S)
$$

Furthermore, we could add the case when both the subject and the object
contain negative determiners, such as in~(\ref{ex:aucun2}). This case is
currently not covered by Frigram (though it could be), but is considered
grammatical by French speakers. Finally, in a complete grammar, $ne$
would have to provide types capable of transforming all the other
syntactic valencies.

\begin{exe}
  \ex \label{ex:aucun2} Aucune fourmi n'aime aucun tatou.
\end{exe}

$$
N_{ne_{tv_3}} : (NP\_NEG{=}F \limp NP\_NEG{=}F \limp S) \limp (NP\_NEG{=}T \limp NP\_NEG{=}T \limp S)
$$

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{parse-aucun.pdf}
  \caption{\label{fig:parse-aucun} One of the parse trees assigned to
    sentence (\ref{ex:aucun-obj}) by Frigram/Leopar.}
\end{figure}

However, this is not the only change we have to effect on our grammar in
order to properly handle the paired grammatical words for
negation. Consider the example sentence (\ref{ex:aucun-obj}) and the
parse tree assigned to it by Frigram on Figure~\ref{fig:parse-aucun}. It
is not enough that the embedded noun phrase $aucun\ tatou$ has a type
that tells us it contains a negative determiner. We need this bit of
internal information also at the level of the enclosing noun phrase
$l'odeur\ d'aucun\ tatou$ in which the former noun phrase is embedded
since this is a property of the enclosing noun phrase that is crucial to
its syntactic combinatorics. This presupposes the existence of a
mechanism for propagating this kind of information up the parse tree. In
the case of our ACG, this means that all the functions which modify
phrases must be aware whether or not their argument contains a negative
determiner and to convey this information in its result type as
well. Moreover, since the negative determiner can also come from a
complement of a prepositional phrase, the types of prepositions will
need to take into account the presence of negative determiners in both
their complements and the phrases they modify.

$$
N_{que_1} : (NP\_NEG{=}F \limp S) \limp N\_NEG{=}F \limp N\_NEG{=}F
$$
$$
N_{que_2} : (NP\_NEG{=}F \limp S) \limp N\_NEG{=}T \limp N\_NEG{=}T
$$
$$
N_{de_1} : NP\_NEG{=}F \limp N\_NEG{=}F \limp N\_NEG{=}F
$$
$$
N_{de_2} : NP\_NEG{=}F \limp N\_NEG{=}T \limp N\_NEG{=}T
$$
$$
N_{de_3} : NP\_NEG{=}T \limp N\_NEG{=}F \limp N\_NEG{=}T
$$

If we suppose the presence of the appropriate lexical items typed as in
the examples of~\ref{ssec:acg}, we have a type system in which we can
type the syntactic terms corresponding to the sentences
(\ref{ex:good-neg-double}), (\ref{ex:good-neg-embed}) and
(\ref{ex:good-neg-rel}) while making it impossible to type the
structures corresponding to sentences (\ref{ex:bad-neg-noneg}) and
({\ref{ex:bad-neg-rel}}).

In sentence (\ref{ex:bad-neg-noneg}), we have no constant
$N_{ne_{tv_?}}$ for $ne$ which would yield a verb capable of consuming
two noun phrases that contain no negative determiners.

In sentence (\ref{ex:bad-neg-rel}), there are multiple problems with
typing. First, $N_{chasse}$ requires its first argument to be of type
$NP\_NEG{=}F$, a noun phrase with no negative determiner. This means
that $\lambda^{\circ} x.\ N_{chasse}\ x\ y$ has type $NP\_NEG{=}F \limp
S$ which makes it an invalid argument for the function
$N_{aucun}\ N_{loup}$, which has type $(NP\_NEG{=}T \limp S) \limp
S$. Even if we were able to assign it a type compatible with
$N_{que_1}$, the final relative clause would map $N_{tatou}$ to another
term of type $N\_NEG{=}F$, which has no ``free'' negative
determiners. $N_{le_1}$ will map this to a term typed $(NP\_NEG{=}F
\limp S) \limp S$. Finally, this term is not capable of taking
$(\lambda^{\circ} x.\ N_{ne_{iv}}\ N_{court}\ x)$ as its argument, since
its type is $NP\_NEG{=}T \limp S$.

\begin{exe}
  \ex \label{ex:good-neg-double} Aucune fourmi n'aime aucun tatou. \\
      $(N_{aucune}\ N_{fourmi})\ (\lambda^{\circ} x.\ (N_{aucun}\ N_{tatou})\ (\lambda^{\circ} y.\ N_{ne_{tv_3}}\ N_{aime}\ x\ y))$
  \ex \label{ex:good-neg-embed} Jean n'aime l'odeur d'aucun tatou. \\
      $(N_{le_2}\ (N_{de_3}\ (N_{aucun}\ N_{tatou})\ N_{odeur}))\ (\lambda^{\circ} y.\ N_{ne_{tv_2}}\ N_{aime}\ N_{Jean}\ y)$
  \ex \label{ex:good-neg-rel} Le tatou qu'aucun loup ne chasse court. \\
      $(N_{le_1}\ (N_{que_1}\ (\lambda^{\circ} y.\ (N_{aucun}\ N_{loup})\ (\lambda^{\circ} x.\ N_{ne_{tv_1}}\ N_{chasse}\ x\ y))\ N_{tatou}))\ (\lambda^{\circ} x.\ N_{court}\ x)$
  \ex * \label{ex:bad-neg-noneg} Jean n'aime le tatou. \\
      * $(N_{le_1}\ N_{tatou})\ (\lambda^{\circ} y.\ N_{ne_{tv_?}}\ N_{aime}\ N_{Jean}\ y)$
  \ex * \label{ex:bad-neg-rel} Le tatou qu'aucun loup chasse ne court. \\
      * $(N_{le_1}\ (N_{que}\ (\lambda^{\circ} y.\ (N_{aucun}\ N_{loup})\ (\lambda^{\circ} x.\ N_{chasse}\ x\ y))\ N_{tatou}))\ (\lambda^{\circ} x.\ N_{ne_{iv}}\ N_{court}\ x)$
\end{exe}

There are two things that we would like to highlight about this
grammatical treatment. First off, we had to refine our types to make
them convey more than one piece of information in a single type
(e.g. something is a noun phrase and at the same time it is something
that contains a negative determiner). Second, a property that we wish to
express in the type of a term may be due to one of its subconstituents
and it is therefore necessary to encode the propagation of this property
up the parse tree in the types of all the intervening operators. In our
case, it meant that handling negative determiners and the paired
grammatical word for negation required us to not only define and type
constants for these two categories, but to also change the types of noun
modifiers ($que\ ...$) and prepositions ($de$).


\subsection{Extraction}
\label{ssec:extraction}

We will now turn to another phenomenon, extraction in relative clauses,
focus on one of its properties, the fact that subjects can be extracted
using $qui$ only from the relative clause itself whereas objects can be
extracted using $que$ from embedded clauses contained therein, and
present an implementation of this constraint by way of types in an
ACG. The solution that we will show was published in
\cite{pogodalla2012controlling}. Our presentation is slightly adapted to
conform to the patterns seen in previous examples and it does not use
dependent types as they are not essential to this specific constraint.

The key idea in the treatment presented below is to give a special type
to extracted constituents. We will thus have two types for noun phrases:
$NP\_VAR{=}T$, which stands for ``empty'' noun phrases introduced by
extraction, and $NP\_VAR{=}F$, which stands for the other noun phrases,
i.e.  proper names and nouns with determiners. The grammar gives no
constant capable of constructing a value of type $NP\_VAR{=}T$. The only
way to obtain a term with this type is to abstract over a variable
having the type. Then, the types of relative pronouns end up being
$(NP\_VAR{=}T \limp S) \limp N \limp N$, ensuring that the incomplete
clause given as the first argument is a $\lambda$-abstraction whose
argument has type $NP\_VAR{=}T$.

Once we have this kind of information in the type system, we can start
distinguishing clauses containing rooted extraction, where a subject or
object is extracted directly from the relative clause, embedded
extraction, where an object is extracted from a clause embedded inside
the relative clause, or no extraction. Types of functions which produce
values of type $S$ will be sensitive to the presence of extracted
variables in their noun phrase and clausal arguments.

$$
E_{dort_1} : NP\_VAR{=}F \limp S\_EXT{=}NO
$$
$$
E_{dort_2} : NP\_VAR{=}T \limp S\_EXT{=}ROOT
$$
$$
E_{aime_1} : NP\_VAR{=}F \limp NP\_VAR{=}F \limp S\_EXT{=}NO
$$
$$
E_{aime_2} : NP\_VAR{=}T \limp NP\_VAR{=}F \limp S\_EXT{=}ROOT
$$
$$
E_{aime_3} : NP\_VAR{=}F \limp NP\_VAR{=}T \limp S\_EXT{=}ROOT
$$
$$
E_{dit\ que_1} : NP\_VAR{=}F \limp S\_EXT{=}NO \limp S\_EXT{=}NO
$$
$$
E_{dit\ que_2} : NP\_VAR{=}T \limp S\_EXT{=}NO \limp S\_EXT{=}ROOT
$$
$$
E_{dit\ que_3} : NP\_VAR{=}F \limp S\_EXT{=}ROOT \limp S\_EXT{=}EMB
$$
$$
E_{dit\ que_4} : NP\_VAR{=}F \limp S\_EXT{=}EMB \limp S\_EXT{=}EMB
$$

Now all of the infrastructure is in place for us to express the type of
the relative pronouns $qui$ and $que$.

$$
E_{qui} : (NP\_VAR{=}T \limp S\_EXT{=}ROOT) \limp N \limp N
$$
$$
E_{que_1} : (NP\_VAR{=}T \limp S\_EXT{=}ROOT) \limp N \limp N
$$
$$
E_{que_2} : (NP\_VAR{=}T \limp S\_EXT{=}EMB) \limp N \limp N
$$

The above types ensure that the abstracted variables corresponding to
traces of extracted constituents will be given the type $NP\_VAR{=}T$,
that $qui$ can only be used to extract constituents from root positions
while $que$ allows for extraction from both root and embedded
positions.\footnote{NB: The constraint that $qui$ can only extract
  subjects and that $que$ can only extract objects is not enforced
  here. Likewise, this fragment does not handle multiple extraction.}

We also note that the splitting of the clause type $S$ into three finer
types $S\_EXT={NO}$, $S\_EXT{=}ROOT$ and $S\_EXT{=}EMB$ means that types
that work with $S$ regardless of its extraction status will have to
provide alternatives for all the possible variants.

$$
E_{le_1} : N \limp ((NP\_VAR{=}F \limp S\_EXT{=}NO) \limp S\_EXT{=}NO)
$$
$$
E_{le_2} : N \limp ((NP\_VAR{=}F \limp S\_EXT{=}ROOT) \limp S\_EXT{=}ROOT)
$$
$$
E_{le_3} : N \limp ((NP\_VAR{=}F \limp S\_EXT{=}EMB) \limp S\_EXT{=}EMB)
$$

While we have introduced the notion of an $NP\_VAR{=}T$ for describing
traces, we can cover another element of our running fragment, the
preposition $de$, and enforce the constraint that $qui$/$que$ cannot
extract prepositional complements by providing only a single type for
$de$ that accepts only unmoved noun phrases as complements.

$$
E_{de} : NP\_VAR{=}F \limp N \limp N
$$

Given this type system, we can assign the type $S\_EXT{=}NO$ to the term
expressing the syntactic structure of (\ref{ex:good-ext}) but we cannot
a find a typable term for sentence (\ref{ex:bad-ext}), where $qui$ is
used to extract a subject from a clause embedded in a relative clause.
The problem with sentence (\ref{ex:bad-ext}) is that the type of the
relative clause is $NP\_VAR{=}T \limp S\_EXT{=}EMB$ which is not a valid
argument for any constant representing the relative pronoun
$qui$. Typing the variable $t$ with $NP\_VAR{=}F$ instead of
$NP\_VAR{=}T$ and using $E_{aime_1}$ instead of $E_{aime_2}$ would not
help, since the resulting relative clause would then have type
$NP\_VAR{=}F \limp S\_EXT{=}NO$, which would not be admitted by any
relative pronoun.

\begin{exe}
  \ex * \label{ex:bad-ext} Le tatou qui Jean dit que \_ aime Marie dort. \\
      * $(E_{le_1}\ (E_{qui_?}\ (\lambda^{\circ} t.\ E_{dit\ que_3}\ E_{Jean}\ (E_{aime_2}\ t\ E_{Marie}))\ E_{tatou}))\ (\lambda^{\circ} x.\ E_{dort_1}\ x)$
  \ex \label{ex:good-ext} Le tatou que Jean dit que Marie aime \_ dort. \\
      $(E_{le_1}\ (E_{que_2}\ (\lambda^{\circ} t.\ E_{dit\ que_3}\ E_{Jean}\ (E_{aime_3}\ E_{Marie}\ t))\ E_{tatou}))\ (\lambda^{\circ} x.\ E_{dort_1}\ x)$
\end{exe}

As was the case with our treatment of negation, we refined our familiar
types by tacking on new kinds of information and defined rules for how
these pieces of information percolate up the parse tree all the way to
the topmost level still pertinent for ensuring grammaticality. We will
now look at another similar example.


\subsection{Agreement}

Finally, we will cover the phenomenon of number agreement. This
motivates a straightforward refinement of our types, where all noun and
noun phrase types carry number information.

$$
A_{Marie} : NP\_NUM{=}SG
$$
$$
A_{tatou} : N\_NUM{=}SG
$$
$$
A_{tatous} : N\_NUM{=}PL
$$
$$
A_{le} : N\_NUM{=}SG \limp ((NP\_NUM{=}SG \limp S) \limp S)
$$
$$
A_{les} : N\_NUM{=}PL \limp ((NP\_NUM{=}PL \limp S) \limp S)
$$
$$
A_{dort} : NP\_NUM{=}SG \limp S
$$
$$
A_{dorment} : NP\_NUM{=}PL \limp S
$$
$$
A_{aime_1} : NP\_NUM{=}SG \limp NP\_NUM{=}SG \limp S
$$
$$
A_{aime_2} : NP\_NUM{=}SG \limp NP\_NUM{=}PL \limp S
$$
$$
A_{aiment_1} : NP\_NUM{=}PL \limp NP\_NUM{=}SG \limp S
$$
$$
A_{aiment_2} : NP\_NUM{=}PL \limp NP\_NUM{=}PL \limp S
$$
$$
A_{qui_1} : (NP\_NUM{=}SG \limp S) \limp N\_NUM{=}SG \limp N\_NUM{=}SG
$$
$$
A_{qui_2} : (NP\_NUM{=}PL \limp S) \limp N\_NUM{=}PL \limp N\_NUM{=}PL
$$
$$
A_{de_1} : NP\_NUM{=}SG \limp N\_NUM{=}SG \limp N\_NUM{=}SG
$$
$$
A_{de_2} : NP\_NUM{=}PL \limp N\_NUM{=}SG \limp N\_NUM{=}SG
$$
$$
A_{de_3} : NP\_NUM{=}SG \limp N\_NUM{=}PL \limp N\_NUM{=}PL
$$
$$
A_{de_4} : NP\_NUM{=}PL \limp N\_Pl \limp N\_NUM{=}PL
$$


Given the above signature, we can assign the type $S$ to the syntactic
term of sentence (\ref{ex:good-agr}) but not to the one of sentence
(\ref{ex:bad-agr}). In the latter sentence, the type of the variable $t$
has to be $NP\_NUM{=}SG$ since it is used as an argument to $A_{dort}$. This
means that the relative clause $dort$ has type $NP\_NUM{=}SG \limp
S$. However, there is no type for $qui$ which would allow us to modify
the plural noun $A_{tatous}$ of type $N\_NUM{=}PL$ by a relative clause with a
missing singular noun phrase, which has the type $NP\_NUM{=}SG \limp S$.

\begin{exe}
  \ex \label{ex:good-agr} Les tatous de Marie dorment. \\
      $(A_{les}\ (A_{de}\ A_{Marie}\ A_{tatous}))\ (\lambda^{\circ} x.\ A_{dorment}\ x)$
  \ex * \label{ex:bad-agr} Marie aime les tatous qui dort. \\
      * $(A_{les}\ (A_{qui_?}\ (\lambda^{\circ} t.\ A_{dort}\ t)\ A_{tatous}))\ (\lambda^{\circ} y.\ A_{aime_2}\ A_{Marie}\ y)$
\end{exe}

Once again, we have expressed a linguistic constraint by refining our
types, augmenting them with a new bit of information, we constrained the
arguments of functions using these new types and we have described how
this new type information propagates from the given arguments to the
produced values.

In the next subsection, we will see what it takes to enforce all of the
constraints above (negation, extraction and agreement) at the same time,
which will give us some idea on how to go about building a grammar which
handles a wide variety of such phenomena.


\subsection{Putting It All Together}

In a grammar that handles negation, extraction and agreement, our types
will need to carry all of the refinements we introduced before. To
recapitulate, $NP$ will be parameterized by whether or not it contains a
negative determiner ($\_\textcolor{red}{NEG{=}T}$ and
$\_\textcolor{red}{NEG{=}F}$ respectively), whether or not it is a trace
from an extraction ($\_\textcolor{green}{VAR{=}T}$ and
$\_\textcolor{green}{VAR{=}F}$) and its number
($\_\textcolor{blue}{NUM{=}SG}$ or $\_\textcolor{blue}{NUM{=}PL}$). For
$N$, we have the same refinements except for $\textcolor{green}{VAR}$
since extraction only moves $NP$s. Clauses, of type $S$, will be
discriminated based on the level of extraction within them
($\_\textcolor{green}{EXT{=}NO}$, $\_\textcolor{green}{EXT{=}ROOT}$ and
$\_\textcolor{green}{EXT{=}EMB}$).

The individual types will have to respect all of the constraints at the
same time, each one being a complete specification of a possible
situation (complete w.r.t. the features listed above). To see and
appreciate what this means in practice, we give the types for the
preposition $de$:

$$
C_{de_1} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}SG})
$$
$$
C_{de_2} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}PL})
$$
$$
C_{de_3} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}SG})
$$
$$
C_{de_4} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}PL})
$$
$$
C_{de_5} : (NP\_\textcolor{red}{NEG{=}T}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}SG})
$$
$$
C_{de_6} : (NP\_\textcolor{red}{NEG{=}T}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}PL})
$$
$$
C_{de_7} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}SG})
$$
$$
C_{de_8} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}PL})
$$
$$
C_{de_9} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}SG})
$$
$$
C_{de_{10}} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}PL})
$$
$$
C_{de_{11}} : (NP\_\textcolor{red}{NEG{=}T}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}SG}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}SG})
$$
$$
C_{de_{12}} : (NP\_\textcolor{red}{NEG{=}T}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}PL}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}PL})
$$

This highlights two important problems with the current approach. One is
the untenable complexity. Even though the three mechanisms are
completely independent of each other, here we are forced to refer to all
of them in every type. It is no longer easy to glance at the types
assigned to $de$ and see how it behaves with respect to, for example,
the mechanism of negation (compare with the type assignments we gave to
$de$ in previous subsections). Another cause of unreadability of this
signature is due to the sheer number of types on display, which leads us
to our second issue.

The signature demonstrates an exponential growth in the number of typed
constants included. In cases where the behaviors of a wordform across
the different mechanisms are independent, the complete type signature
will need to provide a type for every combination of situations in all
of the mechanisms. Assuming that every mechanism will assign on average
more than one type to every wordform, the number of the wordform's types
in the final signature will be exponential w.r.t. the number of
different mechanisms handled.

We can try making the enumeration of these types easier for the grammar
author by introducing metavariables for the values of the features. This
means that we could write the following 3 templates instead of the 12
types above:

$$
\forall \textcolor{blue}{x}, \textcolor{blue}{y} \in \{\textcolor{blue}{SG}, \textcolor{blue}{PL}\}
$$
$$
C_{de_{1(\textcolor{blue}{x}, \textcolor{blue}{y})}} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}x}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}y}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}y})
$$
$$
C_{de_{2(\textcolor{blue}{x}, \textcolor{blue}{y})}} : (NP\_\textcolor{red}{NEG{=}F}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}x}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}y}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}y})
$$
$$
C_{de_{3(\textcolor{blue}{x}, \textcolor{blue}{y})}} : (NP\_\textcolor{red}{NEG{=}T}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}x}) \limp (N\_\textcolor{red}{NEG{=}F}\_\textcolor{blue}{NUM{=}y}) \limp (N\_\textcolor{red}{NEG{=}T}\_\textcolor{blue}{NUM{=}y})
$$

This is getting more concise and easier to comprehend. We can take this
approach further by not only quantifying over variables but also
computing some of the values using functions:

$$
\forall \textcolor{blue}{x}, \textcolor{blue}{y} \in\{\textcolor{blue}{SG}, \textcolor{blue}{PL}\}, \forall \textcolor{red}{p}, \textcolor{red}{q} \in \{\textcolor{red}{F}, \textcolor{red}{T}\}, \textcolor{red}{p \land q} \neq \textcolor{red}{T}
$$
$$
C_{de(\textcolor{blue}{x}, \textcolor{blue}{y}, \textcolor{red}{p}, \textcolor{red}{q})} : (NP\_\textcolor{red}{NEG{=}p}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}x}) \limp (N\_\textcolor{red}{NEG{=}q}\_\textcolor{blue}{NUM{=}y}) \limp (N\_\textcolor{red}{NEG{=}(p \lor q)}\_\textcolor{blue}{NUM{=}y})
$$

The above is vastly more concise and manages to convey the intent more
clearly (the intent being that it cannot be the case that there are
negative determiners in both the complement noun phrase and the modified
noun and that the value of the result's \textcolor{red}{NEG} feature is
always the disjunction of the \textcolor{red}{NEG} features of the two
arguments).

As of right now, we have introduced this kind of rule only as
meta-notation. This means that the real underlying grammar that will end
up being used by some algorithm still suffers from the same exponential
cardinality of the signature. Fortunately, there exists a well-studied
construction in type theory that lets us assign to terms generic types
such as the one defined in the template above. This construction is the
\emph{dependent product} and its utility in ACGs has already been
noted.\footnote{See \cite{de2007two} for its introduction into the
  domain of ACGs, \cite{de2007type} for a small example grammar, which
  handles the interaction between number and gender agreement and
  coordination, and finally see \cite{pogodalla2012controlling} for
  using dependent types to handle several constraints regarding
  extraction (including the one we covered in~\ref{ssec:extraction}).}

While extending the type system by including dependent types is
definitely useful and it helps to solve the problem of having our
signatures grow exponentially in size, it does not solve our problem
with the complexity of the new grammar. Granted, the new meta-type
written in the dependent style is more readable and easier to reason
about than the enumeration of 12 basic types we presented at the
beginning of this subsection, however, the result is still complex in
that it forces us to describe the behavior of three independent
mechanisms in one place. This one place happens to be a wordform which
has seemingly little connection to these mechanisms, the preposition
$de$. We can therefore imagine that it will participate in many other
phenomena and that the type we assign to it will grow beyond our
abilities to reason about it effectively.

Furthermore, the current notation does not even make it obvious that
these are independent mechanisms. One has to examine the type to make
sure that there is no interaction. In the next two subsections, we will
devote ourselves to investigating ways of disentangling these.


\subsection{Intersections of Types}

Our goal is to split the types into smaller parts that only talk about
features belonging to one of the three mechanisms covered above. This
means that we would like to take some phrase about which we know a lot
of disparate pieces of information (presence of negative determiner,
trace status, number\ldots), having type
e.g. $NP\_\textcolor{red}{NEG{=}T}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}$,
and use it as an argument for some function which only cares about one
channel of information and would have a type like
$NP\_\textcolor{red}{NEG{=}T} \limp N\_\textcolor{red}{NEG{=}F} \limp
N\_\textcolor{red}{NEG{=}T}$. To make a type such as
$NP\_\textcolor{red}{NEG{=}T}\_\textcolor{green}{VAR{=}F}\_\textcolor{blue}{NUM{=}PL}$
be a legal argument to a function which expects an argument of type
$NP\_\textcolor{red}{NEG{=}T}$, we will need to introduce some notion of
\emph{subtyping}.

The types we have been showing up to now propose a very straightforward
notion of subtyping. All of the types can be seen as conjunctions of
properties where $\_$ (underscore) is the conjunction operator. A type
$\tau$ can then be seen as a subtype of type $\sigma$ whenever the
conjuncts of $\tau$ form a superset of the conjuncts of $\sigma$. This
is not unlike the structural typing of records in OCaml (CITE SOME OCAML
SPEC HERE), where a record type is a subtype of another record type if
the former provides, among others, all the fields provided by the latter
and gives them the same type. In our case, one type is a subtype of
another if the former gives, among others, all the properties given by
the latter. This notion of subtyping can be expressed by adding the rule
of inference in Figure~\ref{fig:type-superset} to our type
system.\footnote{In the rule \ldots (talk about $\land$ instead of $\_$
  (underscore) here...)}

\begin{figure}
  \begin{prooftree}
    \AxiomC{$\Gamma_l; \Gamma_i \vdash_\Sigma M : A_1 \land \ldots A_{i-1} \land A_i \land A_{i+1} \ldots \land A_n$}
    \RightLabel{(subtype)}
    \UnaryInfC{$\Gamma_l; \Gamma_i \vdash_\Sigma M : A_1 \land \ldots A_{i-1} \land A_{i+1} \ldots \land A_n$}
  \end{prooftree}
  \caption{\label{fig:type-superset} A rule of inference giving us
    subtyping for conjunctions of atomic types. For readability, we use
    $\land$ instead of $\_$ (underscore) to designate the conjunction of
    properties.}
\end{figure}

(Talk about whether or not this is a conservative extension...)

With the new subtyping rule, we can

$$
S_{tatou} : N \land NEG{=}F \land NUM{=}SG
$$
$$
S_{le_{NEG_1}} : (N \land NEG{=}F) \limp (((NP \land NEG{=}F) \limp S) \limp S)
$$
$$
S_{le_{NEG_2}} : (N \land NEG{=}T) \limp (((NP \land NEG{=}T) \limp S) \limp S)
$$
$$
S_{le_{NUM}} : (N \land NUM{=}SG) \limp (((NP \land NUM{=}SG) \limp S) \limp S)
$$
$$
S_{le_{VAR}} : N \limp (((NP \land VAR{=}F) \limp S) \limp S)
$$

\ldots


\subsection{Intersections of Languages}

We will now turn to another solution to the complexity of grammars
implementing multiple independent mechanisms.

When ACGs are discussed, one of the oft-praised properties is
compositionality. Let us consider a situation where we have three
signatures $\Sigma_1$, $\Sigma_2$ and $\Sigma_3$, their respective
distinguished types $S_1$, $S_2$ and $S_3$ and two lexicons
$\mathcal{L}_{12} : \Sigma_1 \to \Sigma_2$ and $\mathcal{L}_{23} :
\Sigma_2 \to \Sigma_3$ such that $\mathcal{L}_{12}(S_1) = S_2$ and
$\mathcal{L}_{23}(S_2) = S_3$. (TODO: INCLUDE A DIAGRAM OF THIS
SITUATION)

We can now define languages $A_1$, $A_2$ and $A_3$ where $A_i$ is the
set of terms from $\Lambda(\Sigma_i)$ that have the type $S_i$. Such
languages, which are defined only in terms of a signature and some
distinguished type, are called \emph{abstract languages} in the
terminology of ACGs. They consist of terms whose structure is
constrained only by the types assigned by the signature.

Further than that, we can look at the languages $\mathcal{L}_{12}(A_1)$
and $\mathcal{L}_{23}(A_2)$, images of the abstract languages given by
the lexicons. Since the lexicons are homomorphic mappings w.r.t. the
types, all elements of $\mathcal{L}_{12}(A_1)$ will have the type
$\mathcal{L}_{12}(S_1) = S_2$ and will therefore also belong to $A_2$
(similarly for $\mathcal{L}_{23}(A_2)$ and $A_3$). These new languages,
which are formed by mapping an ACG abstract language using a lexicon,
are called \emph{object languages}. Their terms are constrained not only
by the types of the signature they are built upon but also by the type
system of the abstract signature.

Finally, we can also take the composition of lexicons $\mathcal{L}_{13}
= \mathcal{L}_{23} \circ \mathcal{L}_{12}$, which itself is also a
lexicon, and map it over $A_1$. This gives us a language
$\mathcal{L}_{13}(A_1) = \mathcal{L}_{23}(\mathcal{L}_{12}(A_1))$ which
is at the same time the image of the object language
$\mathcal{L}_{12}(A_1)$ given by $\mathcal{L}_{23}$ and also the image
of the abstract language $A_1$ given by $\mathcal{L}_{13}$. This points
to an interesting property of ACG languages: they are closed under
transformation by a lexicon.

In the ACG literature, ACGs are defined as quadruples $\mathcal{G} =
\mathopen{<}\Sigma_A, \Sigma_O, \mathcal{L}, S\mathclose{>}$ where
$\Sigma_A$ is the abstract signature, $\Sigma_O$ is the object
signature, $\mathcal{L} : \Sigma_A \to \Sigma_O$ is a lexicon linking
the two and $S$ is a distinguished type of the abstract signature
$\Sigma_A$. From this grammar, we can define two languages. The abstract
language $\mathcal{A}(\mathcal{G}) = \{t \in \Lambda(\Sigma_A) \mid
\ \vdash_{\Sigma_A} t : S\}$ and the object language
$\mathcal{O}(\mathcal{G}) = \mathcal{L}(\mathcal{A}(\mathcal{G})) = \{t
\in \Lambda(\Sigma_O) \mid \exists u \in \mathcal{A}(\mathcal{G}),
\mathcal{L}(u) = t\}$. The composition of two ACGs $\mathcal{G}_1 =
\mathopen{<}\Sigma_1, \Sigma_2, \mathcal{L}_{12}, S_1\mathclose{>}$ and
$\mathcal{G}_2 = \mathopen{<}\Sigma_2, \Sigma_3, \mathcal{L}_{23},
S_2\mathclose{>}$ is defined as $\mathcal{G}_2 \circ \mathcal{G}_1 =
\mathopen{<}\Sigma_1, \Sigma_3, \mathcal{L}_{23} \circ \mathcal{L}_{12},
S_1\mathclose{>}$.

The definition of ACGs given above is quite practical in that it is
complete, it gives a precise account of what set of languages are
generated by ACGs (abstract and object languages for which there exist
the quadruples described above). This contrasts with the presentation of
ACGs that we gave in~\ref{ssec:acg}, where we defined ACGs as
collections of signatures with associated distinguished types and
connected with lexicons. Our definition only introduced the requisite
machinery, but delayed the delimitation of the way languages can be
defined. Leaving this question open will let us answer it now by
providing an extension/generalization of ACGs which will help us
overcome the modularity issues that have been the theme of this
section.\footnote{Incidentally, this was not the reason for the way we
  presented ACGs in~\ref{ssec:acg}. We chose this style as it grouped
  together the information in a more natural way. We approach signatures
  as descriptions of domains (strings, syntax, semantics), each one
  being associated with a distinguished type telling us which objects
  the domain has set out to describe (strings, sentences, truth
  values). Having distinguished types directly attached to signatures
  allows us to omit them from the ACG quadruples. However, this only
  leaves us with triples containing the two signatures and the lexicon
  connecting them, and since we can infer the signatures themselves from
  the domain and the range of the lexicon, we do not have much
  motivation to introduce these tuples. Instead of composing ACGs, we
  compose just the lexicons.

On top of that, we will want to define signatures and lexicons that
yield different object languages (at least some string representation of
the form and some semantic representation of the meaning), which would
mean having at least two different ACGs which are linked (e.g. by
sharing the same abstract signature). However, since these two grammars
are very closely related, we would prefer to reason about our grammar as
\emph{one} system/graph of signatures and lexicons which defines
multiple languages instead of talking about \emph{multiple} grammars
which share some similarities.

TODO: Somehow express that its nice to have the links between the
signatures and lexicons that are shown in diagrams \emph{explicit}
rather than having them \emph{implicit}.}

Before we turn to our proposal, we will quickly recall some of the other
styles of composition used in ACGs (a more detailed exposition can be
found in \cite{pogodalla2012controlling}). Besides making the object
signature of one grammar be the abstract signature of another as we saw
above (TODO: INSERT REFERENCE TO FUTURE DIAGRAM), we can have two
grammars share the same abstract signature allowing us to transduce
between terms in the two object signatures. See Figure~(TODO: INSERT A
DIAGRAM FOR THIS SITUATION) for a diagrammatic example.

These two modes of composition can be fruitfully mixed. Consider (TODO:
INSERT A DIAGRAM), where we introduce a syntactic signature TODO where
different terms are only allowed to yield the same string whenever they
represent different syntactic structures of a syntactically ambiguous
phrase. This means that purely semantic distinctions such as scope
ambiguity have to be moved to a more abstract signature from whose terms
the semantic representation can be derived by a function. This approach
is studied more deeply in \cite{pogodalla2007generalizing}.

Another interesting composition pattern is to take an existing structure
of grammars and add a new abstract signature on top. This signature can
be used to further constrain the items in (TODO: what was before the
abstract-most signature/language). In \cite{pogodalla2012controlling},
this is what the authors use to develop their constraints on
extraction. Each of their constraints is presented as a new abstract
signature constraining the original syntactic signature.

However, a question that the authors do not pose is how to combine all
these constraints. If we were to apply the same pattern repeatedly to
get a structure like the one on (TODO: INSERT A DIAGRAM HERE), we would
have to deal with successively more complex type systems on each
level. This is due to the condition that a lexicon has to be a
homomorphism and thus it must, among other things, always map well-typed
terms to well-typed terms. To illustrate the problem, imagine a
structure such as the one on (TODO) that handles some different
constraint in each of the vertically stacked signatures, including
e.g. agreement. If we tried to add a new abstract signature at the top
to implement another constraint, we would have to ensure that all the
terms we licence in our new abstract signature have a well-typed
counterpart in the existing abstract language. However, this means that
our new signature must make sure that the terms it approves do not break
agreement or any of the previously established constraints. Effectively,
our new signature would be forced to reimplement all of the existing
constraints, rendering in vain all our effort to split up the
constraints into multiple signatures.

What we would like instead is to put the signatures side-by-side such
that they do not depend on each other (as on diagram TODO). However, how
does this actually define a language? We can arrive at the answer by
examining how are the graphical representations of ACGs connected to the
languages they define. We will associate a language to every node of the
graph. Before we begin, we notice that every ACG diagram formed by the
above rules of composition is a directed tree and therefore every node
has at most one parent/predecessor. To the root of the tree, the
abstract-most signature in the graph, we will assign the abstract
language generated by that signature and its distinguished type. To all
the other nodes, we will assign the result of mapping their parent's
language by the lexicon linking the two. By homomorphism of the
lexicons, we can also deduce the distinguished types for all the
non-root nodes. They are the images of their parents' distinguished
types by the connecting lexicons. For all of the nodes, it is true that
every term in the language assigned to the node has the node's
distinguished type.

We can confirm that the above graphical model of defining ACGs is
coherent. First, we can show that our interpretation is sound and so
that any directed tree adorned with signatures on the nodes, compatible
lexicons on the edges and a distinguished type on the root can only
define ACG languages. This is trivial, since the language corresponding
to the root is an ACG abstract language and the class of ACG languages
is closed under transformation by a lexicon and therefore all of the
languages corresponding to the descendants of the root are ACG languages
as well. Second, we can try and show some kind of completeness. However,
there is not much formalization done to describe a ``system of ACGs that
share common signatures'' (something that our generalization will
fix). We can nevertheless show that it is possible to express any ACG
using our graphical model. Given a grammar $\mathcal{G} =
\mathopen{<}\Sigma_A, \Sigma_O, \mathcal{L}, S\mathclose{>}$, we just
produce a tree whose root has the signature $\Sigma_A$ and distinguished
type $S$ and which is connected to a single child node having the
signature $\Sigma_O$ by an edge labelled by the lexicon
$\mathcal{L}$. Then the two nodes of our tree define both the abstract
and the object language defined by $\mathcal{G}$.

We have now given some formal account of how we can read language
definitions in diagrams. Still, we have kept our analysis to the tree
diagrams used in current ACG literature, where every node has at most
one parent. To make the generalization more visible, we will switch from
an algebraic manner of presentation, in which we stated that the
language of a node is equal to an image of its parent's language, to a
calculus-based manner of presentation, in which we will state that the
language of a node is the set of all terms that have an antecedent in
the parent's language. Besides this property, there is also the fact
that every term in a node's language has the node's distinguished
type. This property is trivially true for nodes with parents because of
the homomorphic property of lexicons. However, it is crucial for the
root node for which this is the single defining property of its
language. To sum up, we have that for every node of a tree diagram, its
language is the set of all terms that have the node's distinguished type
and that have an antecedent in the parent's language, if there is
any. This rule is already general enough to handle the case of zero or
one parents, we can easily generalize it to handle an arbitrary count:
\emph{the language of a node is the set of all terms that have the
  node's distinguished type and that have antecedents in the languages
  of all of the node's parents}. If we transpose this back to the
algebraic style of presentation, we say that the language of a node is
the intersection of the set of terms having the node's distinguished
type and the intersection of all its parents' languages.

TODO: In the previous paragraph, add a notice saying that the terms must
be also built on the signature of the node.

TODO: Use the correct term arborescence in the above paragraphs!
% http://en.wikipedia.org/wiki/Arborescence_(graph_theory)

We will now turn to a formal definition of this generalized graph-based
formalism and discuss some its values and properties.

\subsubsection{Graphical Abstract Categorial Grammars}

We define a \emph{graphical abstract categorial grammar} as a quadruple
$\mathcal{G} = \mathopen{<} G, \Sigma, S, {\mathcal{L}} \mathclose{>}$
where $G$ is a directed graph with vertices $V(G)$ and edges $E(G)$,
$\Sigma$ and $S$ are labelings assigning signatures and distinguished
types, respectively, to the vertices in $V$ and ${\mathcal{L}}$ is a
labeling assigning lexicons to the edges in $E$. Furthermore, a
well-formed graphical ACG satisfies the following conditions:

\begin{itemize}
\item $G$ is a directed acyclic graph.
\item For all $(u,v) \in E(G)$, $\mathcal{L}_{(u,v)} : \Sigma_u \to
  \Sigma_v$.
\item For all $(u,v) \in E(G)$, $\mathcal{L}_{(u,v)}(S_u) = S_v$.
\end{itemize}

We will say that a node $u$ is \emph{more abstract} than a node $v$
whenever $(u,v)$ belongs to the strict partial order induced by the
edges of $G$ (i.e. there is a path from $u$ to $v$).

We let the nodes in a graphical ACG $\mathcal{G} = \mathopen{<} G,
\Sigma, S, \mathcal{L} \mathclose{>}$ define two kinds of
languages. The \emph{intrinsic languages} $\mathcal{I}_{\mathcal{G}}$,
which are constrained only by the type signature described in the node
itself, and the \emph{extrinsic languages} $\mathcal{E}_{\mathcal{G}}$,
which are also constrained by type systems in more abstract nodes of the
graph. These two notions correspond to those of abstract languages and
object languages in ACGs.

$$
\mathcal{I}_{\mathcal{G}}(v) = \{t \in \Lambda(\Sigma_v)
\mid\ \vdash_{\Sigma_u} t : S_v\}
$$
$$
\mathcal{E}_{\mathcal{G}}(v) = \mathcal{I}_{\mathcal{G}}(v) \cap
\bigcap_{(u,v) \in E} \mathcal{L}_{(u,v)}(\mathcal{E}_{\mathcal{G}}(u)).
$$

From the above definition, we see that the ACG diagrams we introduced
before are a special case of graphical ACGs where the graph is an
arborescence.

\subsubsubsection{Formal Properties of Graphical ACGs}

Observation: The set of intrinsic languages definable by graphical ACGs
is the same as the set of abstract languages definable by ACGs.

Theorem: The set of extrinsic languages definable by graphical ACGs is
the set of object languages definable by ACGs closed under intersection
and transformation by a lexicon.

Proof:

First, we prove that an extrinsic language can be always constructed
from object languages by intersections and transformations by
lexicons. Let us consider any graphical ACG $\mathcal{G} = \mathopen{<}
G, \Sigma, S, \mathcal{L} \mathclose{>}$ and any topological ordering
$v_1, \ldots, v_n$ of the nodes of $G$ (an ordering such that more
abstract nodes always precede less abstract ones). Before we start, we
will just note that the all the intrinsic languages defined by $v_i$ are
abstract languages which are a special case of object languages.

\begin{itemize}
\item For $v_1$, the case is trivial since its extrinsic language is
  just its intrinsic language as the node $v_1$ has no predecessors in
  $G$.
\item For any other node $v_n$, we look at the definition of an
  extrinsic language and remark that the only operators are intersection
  and application of a lexicon with the operands being the intrinsic
  language of $v_n$ and the extrinsic languages of more abstract
  nodes. By induction hypothesis, we have that all of the operands have
  been constructed from object languages using intersections and
  transformations by lexicons and therefore even the extrinsic language
  defined by $v_n$ has this property.
\end{itemize}


Now we have to prove the converse. We do so by showing that extrinsic
languages contain object languages and are closed under intersection and
transformation by a lexicon.

\begin{itemize}
\item Let $L$ be an object language defined by the ACG $\mathopen{<}
  \Sigma_A, \Sigma_O, \mathcal{L}, S\mathclose{>}$. We take the graph
  $G$ with $V(G) = \{A, O\}$ and $E(G) = \{(A,O)\}$, we label the nodes
  $A$ and $O$ with $\Sigma_A$ and $\Sigma_O$ as signatures and $S$ and
  $\mathcal{L}(S)$ as distinguished types respectively. Finally, we
  label the edge $(A,O)$ with $\mathcal{L}$ and we have a graphical ACG
  in which the node $O$ defines the extrinsic language $L$.

\item Let $L_1$ and $L_2$ be two extrinsic languages defined by the
  nodes $u_1$ and $u_2$ respectively belonging to two graphical ACGs. We
  can construct a new graphical ACG by taking the union of the two ACGs
  (union of graphs (union of vertices and edges) and unions of the
  labelings).\footnote{Assuming, without loss of generality, that the
    sets of vertices of the two graphs are disjoint.} We then add a new
  node $v$ which will define the language $L_1 \cap L_2$.

  The signature labeling $v$ will be the union of the two signatures
  labeling $u_1$ and $u_2$ such that the intrinsic language of $v$ will
  be a superset of the intrinsic languages, and by extension the
  extrinsic languages, of both $u_1$ and $u_2$. Either of the two
  distinguished types that label $u_1$ or $u_2$ can be used to label $v$
  (if they are actually different, then the intersection is trivially
  empty). Finally, we will add two edges leading from $u_1$ and $u_2$ to
  $v$, both labeled with the identity lexicon. We will now show that
  the extrinsic language of $v$ in the newly formed graphical ACG
  $\mathcal{G}'$ is $L_1 \cap L_2$.

  \begin{align*}
    \mathcal{E}_{\mathcal{G}'}(v) &= \mathcal{I}_{\mathcal{G}'}(v) \cap
    \mathcal{L}_{(u_1,v)}(\mathcal{E}_{\mathcal{G}'}(u_1)) \cap
    \mathcal{L}_{(u_2,v)}(\mathcal{E}_{\mathcal{G}'}(u_2)) \\ &=
    \mathcal{I}_{\mathcal{G}'}(v) \cap \mathcal{E}_{\mathcal{G}'}(u_1) \cap
    \mathcal{E}_{\mathcal{G}'}(u_2) \\ &= \mathcal{E}_{\mathcal{G}'}(u_1)
    \cap \mathcal{E}_{\mathcal{G}'}(u_2) \\ &=
    \mathcal{E}_{\mathcal{G}}(u_1) \cap \mathcal{E}_{\mathcal{G}}(u_2) \\ &=
    L_1 \cap L_2
  \end{align*}

  The simplifications use the facts that: the two new lexicons are
  identities, that the intrinsic language of $v$ is a superset of the
  extrinsic languages of $u_1$ and $u_2$, that the extrinsic languages
  of $u_1$ and $u_2$ are the same in the new graphical ACG as in the two
  original graphical ACGs and finally that $L_1$ and $L_2$ are the
  extrinsic languages defined by $u_1$ and $u_2$.

\item Let $L \subset \Lambda(\Sigma_A)$ be an extrinsic language and
  $\mathcal{L} : \Sigma_A \to \Sigma_O$ a lexicon from its signature
  $\Sigma_A$ to the signature $\Sigma_O$. We will show that the language
  $\mathcal{L}(L) \subset \Lambda(\Sigma_O)$ is also an extrinsic
  language. Let $u$ be the node that defines the extrinsic language $L$
  in some graphical ACG $\mathcal{G}$. We construct a new graphical ACG
  by adding a new vertex $v$ to $\mathcal{G}$ labeled with the signature
  $\Sigma_O$ and the distinguished type $\mathcal{L}(S)$ where $S$ is
  the distinguished type labeling $u$. To this graph, we conjoin the
  edge $(u,v)$ and label it with $\mathcal{L}$. Now we verify that the
  extrinsic language of $v$ is the language $\mathcal{L}(L).$

  \begin{align*}
    \mathcal{E}_{\mathcal{G}'}(v) &= \mathcal{I}_{\mathcal{G}'}(v) \cap
    \mathcal{L}(\mathcal{E}_{\mathcal{G}'}(u)) \\
    &= \mathcal{L}(\mathcal{E}_{\mathcal{G}'}(u)) \\
    &= \mathcal{L}(\mathcal{E}_{\mathcal{G}}(u)) \\
    &= \mathcal{L}(L)
  \end{align*}


\end{itemize}



Corollary: The set of extrinsic languages definable by graphical ACGs is
the same as the set of object languages definable by ACGs if and only if
the set of object languages definable by ACGs is closed under
intersection.
