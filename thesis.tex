\documentclass{article}

%% \usepackage{sdrt}
\usepackage{gb4e}
\usepackage{bussproofs}

%% Linear implication from Alessio Guglielmi
%% https://groups.google.com/forum/?fromgroups=#!topic/comp.text.tex/0B4C3F_BsVI
\def\limp {\mathbin{{-}\mkern-3.5mu{\circ}}}

\begin{document}

\author{Ji\v{r}\'{i} Mar\v{s}\'{i}k}
\title{Master Thesis}
\maketitle

\section{Preliminaries}

\subsection{Motivation}

To develop applications capable of understanding natural language, we
have to analyze the discourse as a complex structure. The structure of
the discourse and the rhetorical relations between its constituent
propositions have shown to have an important effect on anaphora and on
the semantic content of the discourse.

Let us consider these examples from \cite{asher2003logics}.

\begin{exe}
  \ex \label{narxexp-nar} \begin{xlist}
    \ex \label{narxexp-nar-a} Max fell.
    \ex \label{narxexp-nar-b} John helped him up.
  \end{xlist}
  \ex \label{narxexp-exp} \begin{xlist}
    \ex \label{narxexp-exp-a} Max fell.
    \ex \label{narxexp-exp-b} John pushed him.
  \end{xlist}
\end{exe}

In (\ref{narxexp-nar}), we intuitively recognize that the second
proposition (\ref{narxexp-nar-b}) serves as a narrative continuation of
(\ref{narxexp-nar-a}), whereas in (\ref{narxexp-exp}), the proposition
(\ref{narxexp-exp-b}) serves as an explanation to the event described in
(\ref{narxexp-exp-a}). This distinction has interesting consequences as
to what we can infer from these two discourse excerpts. In the case of
(\ref{narxexp-nar}), we can infer that the event described in
(\ref{narxexp-nar-b}) occurred after the event described in
(\ref{narxexp-nar-a}). In the latter case (\ref{narxexp-exp}), we can
conversely infer that the events happened in the opposite order. We feel
that a complete understanding of the two examples above entails
inferring the correct temporal order and we would welcome a principled
way to make this kind of decisions.

\begin{exe}
  \ex \label{salmon} \begin{xlist}
    \ex Max had a lovely evening last night.
    \ex He had a great meal.
    \ex \label{salmon-here} He ate salmon.
    \ex He devoured lots of cheese.
    \ex He then won a dancing competition.
  \end{xlist}
\end{exe}

%% \sdrtree{
%%  &                      & \LAB{Max had a lovely evening} \\
%%           & \LAB{He had a great meal} & & \LAB{He won a dancing competition} \\
%% \LAB{He ate salmon} &     & \LAB{He devoured cheese}
%% }

The discourse in (\ref{salmon}) and its structure in (TODO) demonstrate
another feature of discourse structures. First off, seeing the
hiearchical structure of the discourse gives us important information
about the granularity of the descriptions given in the individual
propositions, information that could be useful for performing tasks such
as text summarization.

Furthermore, the discourse structure has grammatical consequences. It is
thanks to our knowledge of the discourse structure that we can predict
that a proposition like ``It was a beautiful pink.''  could not
coherently follow our excerpt. SDRT, \cite{asher2003logics}, the theory
of discourse structure that we adhere to, would not license a discourse
structure in which the new proposition connects to (\ref{salmon-here})
as its \emph{Elaboration}. As a consequence, it states that the pronoun
``it'' cannot have the salmon as its antecedent, which is a constraint
that is not enforced by prior theories of discourse such as DRT.

\begin{exe}
  \ex \label{diaimp} \begin{xlist}
    \ex A: Smith doesn't seem to have a girlfriend.
    \ex \label{diaimp-b} B: He's been paying lots of visits to New York lately.
  \end{xlist}
\end{exe}

In the example dialogue (\ref{diaimp}), based on the prosody of
proposition (\ref{diaimp-b}) the discourse structure would link the two
propositions with an \emph{Evidence} or \emph{Counter-evidence}
relation. Knowing this structure would allow us to either infer that B
believes that Smith has a girlfriend in New York or that Smith doesn't
have a girlfriend because he is too busy in New York.

Based on the findings in \cite{asher2003logics}, we surmise that a clear
picture of the discourse structure is essential to capturing the
intended meaning of a discourse.

To support our approach, we will stand on the shoulders of many giants,
the first of them being Richard Montague. As in his approach
\cite{montague1973proper}, we will assign functions and values as
denotations of wordforms and use function application to compose them
together to yield the denotations of phrases, propositions and
discourses. To account for the discourse-level phenomena, we will defer
to the theories of DRT \cite{kamp1993discourse} and SDRT
\cite{asher2003logics}.

The grammatical formalism of our choice for this task will be the
Abstract Categorial Grammars (ACG) \cite{de2001towards}. This framework
lets use lambda calculus to express the syntax-semantics interface in a
fashion similar to Montague's. Furthermore, elegant techniques for using
ACGs to implemenent DRT and SDRT using continuations have been
discovered \cite{de2006towards} \cite{asher2011sdrt}
\cite{asher2011montagovian}.

Finally, our work will be supported by the existence of a corpus
annotated with the rhetorical relations of SDRT, the Annodis corpus
\cite{afantenos2012empirical}.


\subsection{Abstract Categorial Grammars}

We present the grammatical framework in which we will develop our
system. Abstract categorial grammars are built upon two mathematical
structures, \emph{(higher-order) signatures} and \emph{lexicons}.

A \textbf{higher-order signature} is a set of elements that we call
\emph{constants}, each of which is associated with a type. Formally, it
is defined as a triple $\Sigma = \mathopen{<}A, C, \tau\mathclose{>}$,
where:
\begin{itemize}
  \item $C$ is the (finite) set of constants
  \item $A$ is a (finite) set of atomic types
  \item $\tau$ is the type associating mapping from $C$ to $\mathcal{T}(A)$,
    the set of types built over $A$
\end{itemize}

In our case, $\mathcal{T}(A)$ is the implicative fragment of linear and
intuitionistic logic with $A$ being the atomic propositions. This means
that $\mathcal{T}(A)$ contains all the $a \in A$ and all the $\alpha \limp
\beta$ and $\alpha \to \beta$ for $\alpha, \beta \in \mathcal{T}(A)$.

A signature $\Sigma = \mathopen{<}A, C, \tau\mathclose{>}$, by itself,
already lets us define an interesting set of structures, that is the set
$\Lambda(\Sigma)$ of \emph{well-typed lambda terms} built upon the
signature $\Sigma$. The set of well-typed terms and their types are
established through the judgment schemas in Figure
\ref{fig:type-judgments}.

\begin{figure}
  \begin{prooftree}
    \AxiomC{$\emptyset, \Gamma_i \vdash_\Sigma c : \tau(c)$ (cons)}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$(x : \alpha), \Gamma_i \vdash_\Sigma x : \alpha$ (l-var)}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\emptyset, (\Gamma_i, x : \alpha) \vdash_\Sigma x : \alpha$ (i-var)}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$(\Gamma_l, x : \alpha), \Gamma_i \vdash_\Sigma t : \beta$}
    \RightLabel{(l-abs)}
    \UnaryInfC{$\Gamma_l, \Gamma_i \vdash_\Sigma \lambda^{\circ} x. t : \alpha \limp \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma_l, (\Gamma_i, x : \alpha) \vdash_\Sigma t : \beta$}
    \RightLabel{(i-abs)}
    \UnaryInfC{$\Gamma_l, \Gamma_i \vdash_\Sigma \lambda x. t : \alpha \to \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma_l, \Gamma_i \vdash_\Sigma t : \alpha \limp \beta$}
    \AxiomC{$\Delta_l, \Delta_i \vdash_\Sigma u : \alpha$}
    \RightLabel{(l-app)}
    \BinaryInfC{$(\Gamma_l, \Delta_l), (\Gamma_i, \Delta_i) \vdash_\Sigma (t\ u) : \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma_l, \Gamma_i \vdash_\Sigma t : \alpha \to \beta$}
    \AxiomC{$\emptyset, \Delta_i \vdash_\Sigma u : \alpha$}
    \RightLabel{(i-app)}
    \BinaryInfC{$\Gamma_l, (\Gamma_i, \Delta_i) \vdash_\Sigma (t\ u) : \beta$}
  \end{prooftree}
  \caption{\label{fig:type-judgments} Type judgment schemas of the
    well-typed lambda terms $\Lambda(\Sigma)$ for a signature $\Sigma =
    \mathopen{<}A, C, \tau\mathclose{>}$}
\end{figure}

The definition of a well-typed lambda term already gives us an
interesting combinatorial structure. To make this structure even more
useful, we often focus ourselves only on terms that have a specific
\emph{distinguished type}. Using this notion of a signature of typed
constants and some distinguished type, we can describe languages of,
e.g. tree-like (and by extension string-like), lambda terms.

The idea of a signature is coupled with that of a \textbf{lexicon},
which is a mapping between two different signatures (mapping the
constants of one into well-typed terms of the other). Formally speaking,
a lexicon $\mathcal{L}$ from a signature $\Sigma_1 = \mathopen{<}A_1, C_1,
\tau_1\mathclose{>}$ (which we call the abstract signature) to a
signature $\Sigma_2 = \mathopen{<}A_2, C_2, \tau_2\mathclose{>}$ (which
we call the object signature) is a pair $\mathopen{<}F, G\mathclose{>}$
such that:
\begin{itemize}
\item $G$ is a mapping from $C_1$ to $\Lambda(\Sigma_2)$ assigning to
  every constant of the abstract signature a term in the object
  signature, which can be understood as its
  interpretation/implementation/realization.
\item $F$ is a mapping from $A_1$ to $\mathcal{T}(A_2)$ which links the
  abstract-level types with the object-level types that they realized
  in.
\item $F$ and $G$ are compatible, meaning that for any $c \in C_1$, we
  have $\vdash_{\Sigma_2} G(c) : \hat{F}(\tau_1(c))$ (we will be using
  $\hat{F}$ and $\hat{G}$ to refer to the homomorphic extensions of $F$
  and $G$ to $\mathcal{T}(A_1)$ and $\Lambda(\Sigma_1)$ respectively).
\end{itemize}

An abstract categorial grammar for us will then be just a collection of
signatures with their distinguished types and lexicons connecting these
signatures. A common pattern will have us using two signatures for the
surface forms of utterances (strings) and their logical forms (logical
propositions) and an abstract signature which is connected to both of
the object signatures via lexicons. Parsing is then just a matter of
inverting the surface lexicon to get the abstract term and then applying
to it the logical lexicon. Generation is symmetric, we simply invert the
logical lexicon and apply the surface lexicon.

\subsubsection{Example ACG}

We will look at an example of an ACG from


$\Sigma_{Synt}$ will be our abstract signature with $A_{Synt} = \{np, n,
s\}$, $C_{Synt} = \{C_{every}, C_{some}, C_{love}, C_{man}, C_{woman}\}$
and $\tau_{Synt}$ as follows:
\begin{itemize}
\item
\end{itemize}

...

\subsection{Interaction Grammars}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
