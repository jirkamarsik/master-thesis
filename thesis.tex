\documentclass{article}

%% \usepackage{sdrt}
\usepackage{gb4e}
\usepackage{bussproofs}

%% Linear implication from Alessio Guglielmi
%% https://groups.google.com/forum/?fromgroups=#!topic/comp.text.tex/0B4C3F_BsVI
\def\limp {\mathbin{{-}\mkern-3.5mu{\circ}}}

\begin{document}

\author{Ji\v{r}\'{i} Mar\v{s}\'{i}k}
\title{Master Thesis}
\maketitle

\section{Preliminaries}

\subsection{Motivation}

To develop applications capable of understanding natural language, we
have to analyze the discourse as a complex structure. The structure of
the discourse and the rhetorical relations between its constituent
propositions have shown to have an important effect on anaphora and on
the semantic content of the discourse.

Let us consider these examples from \cite{asher2003logics}.

\begin{exe}
  \ex \label{narxexp-nar} \begin{xlist}
    \ex \label{narxexp-nar-a} Max fell.
    \ex \label{narxexp-nar-b} John helped him up.
  \end{xlist}
  \ex \label{narxexp-exp} \begin{xlist}
    \ex \label{narxexp-exp-a} Max fell.
    \ex \label{narxexp-exp-b} John pushed him.
  \end{xlist}
\end{exe}

In (\ref{narxexp-nar}), we intuitively recognize that the second
proposition (\ref{narxexp-nar-b}) serves as a narrative continuation of
(\ref{narxexp-nar-a}), whereas in (\ref{narxexp-exp}), the proposition
(\ref{narxexp-exp-b}) serves as an explanation to the event described in
(\ref{narxexp-exp-a}). This distinction has interesting consequences as
to what we can infer from these two discourse excerpts. In the case of
(\ref{narxexp-nar}), we can infer that the event described in
(\ref{narxexp-nar-b}) occurred after the event described in
(\ref{narxexp-nar-a}). In the latter case (\ref{narxexp-exp}), we can
conversely infer that the events happened in the opposite order. We feel
that a complete understanding of the two examples above entails
inferring the correct temporal order and we would welcome a principled
way to make this kind of decisions.

\begin{exe}
  \ex \label{salmon} \begin{xlist}
    \ex Max had a lovely evening last night.
    \ex He had a great meal.
    \ex \label{salmon-here} He ate salmon.
    \ex He devoured lots of cheese.
    \ex He then won a dancing competition.
  \end{xlist}
\end{exe}

%% \sdrtree{
%%  &                      & \LAB{Max had a lovely evening} \\
%%           & \LAB{He had a great meal} & & \LAB{He won a dancing competition} \\
%% \LAB{He ate salmon} &     & \LAB{He devoured cheese}
%% }

The discourse in (\ref{salmon}) and its structure in (TODO) demonstrate
another feature of discourse structures. First off, seeing the
hiearchical structure of the discourse gives us important information
about the granularity of the descriptions given in the individual
propositions, information that could be useful for performing tasks such
as text summarization.

Furthermore, the discourse structure has grammatical consequences. It is
thanks to our knowledge of the discourse structure that we can predict
that a proposition like ``It was a beautiful pink.''  could not
coherently follow our excerpt. SDRT, \cite{asher2003logics}, the theory
of discourse structure that we adhere to, would not license a discourse
structure in which the new proposition connects to (\ref{salmon-here})
as its \emph{Elaboration}. As a consequence, it states that the pronoun
``it'' cannot have the salmon as its antecedent, which is a constraint
that is not enforced by prior theories of discourse such as DRT.

\begin{exe}
  \ex \label{diaimp} \begin{xlist}
    \ex A: Smith doesn't seem to have a girlfriend.
    \ex \label{diaimp-b} B: He's been paying lots of visits to New York lately.
  \end{xlist}
\end{exe}

In the example dialogue (\ref{diaimp}), based on the prosody of
proposition (\ref{diaimp-b}) the discourse structure would link the two
propositions with an \emph{Evidence} or \emph{Counter-evidence}
relation. Knowing this structure would allow us to either infer that B
believes that Smith has a girlfriend in New York or that Smith doesn't
have a girlfriend because he is too busy in New York.

Based on the findings in \cite{asher2003logics}, we surmise that a clear
picture of the discourse structure is essential to capturing the
intended meaning of a discourse.

To support our approach, we will stand on the shoulders of many giants,
the first of them being Richard Montague. As in his approach
\cite{montague1973proper}, we will assign functions and values as
denotations of wordforms and use function application to compose them
together to yield the denotations of phrases, propositions and
discourses. To account for the discourse-level phenomena, we will defer
to the theories of DRT \cite{kamp1993discourse} and SDRT
\cite{asher2003logics}.

The grammatical formalism of our choice for this task will be the
Abstract Categorial Grammars (ACG) \cite{de2001towards}. This framework
lets use lambda calculus to express the syntax-semantics interface in a
fashion similar to Montague's. Furthermore, elegant techniques for using
ACGs to implemenent DRT and SDRT using continuations have been
discovered \cite{de2006towards} \cite{asher2011sdrt}
\cite{asher2011montagovian}.

Finally, our work will be supported by the existence of a corpus
annotated with the rhetorical relations of SDRT, the Annodis corpus
\cite{afantenos2012empirical}.


\subsection{Abstract Categorial Grammars}

We present the grammatical framework in which we will develop our
system. Abstract categorial grammars are built upon two mathematical
structures, \emph{(higher-order) signatures} and \emph{lexicons}.

\subsubsection{Higher-order signatures}

A \textbf{higher-order signature} is a set of elements that we call
\emph{constants}, each of which is associated with a type. Formally, it
is defined as a triple $\Sigma = \mathopen{<}A, C, \tau\mathclose{>}$,
where:
\begin{itemize}
  \item $C$ is the (finite) set of constants
  \item $A$ is a (finite) set of atomic types
  \item $\tau$ is the type associating mapping from $C$ to $\mathcal{T}(A)$,
    the set of types built over $A$
\end{itemize}

In our case, $\mathcal{T}(A)$ is the implicative fragment of linear and
intuitionistic logic with $A$ being the atomic propositions. This means
that $\mathcal{T}(A)$ contains all the $a \in A$ and all the $\alpha \limp
\beta$ and $\alpha \to \beta$ for $\alpha, \beta \in \mathcal{T}(A)$.

A signature $\Sigma = \mathopen{<}A, C, \tau\mathclose{>}$, by itself,
already lets us define an interesting set of structures, that is the set
$\Lambda(\Sigma)$ of \emph{well-typed lambda terms} built upon the
signature $\Sigma$. The set of well-typed terms and their types are
established through the judgment schemas in Figure
\ref{fig:type-judgments}.

\begin{figure}
  \begin{prooftree}
    \AxiomC{$\emptyset, \Gamma_i \vdash_\Sigma c : \tau(c)$ (cons)}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$(x : \alpha), \Gamma_i \vdash_\Sigma x : \alpha$ (l-var)}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\emptyset, (\Gamma_i, x : \alpha) \vdash_\Sigma x : \alpha$ (i-var)}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$(\Gamma_l, x : \alpha), \Gamma_i \vdash_\Sigma t : \beta$}
    \RightLabel{(l-abs)}
    \UnaryInfC{$\Gamma_l, \Gamma_i \vdash_\Sigma \lambda^{\circ} x. t : \alpha \limp \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma_l, (\Gamma_i, x : \alpha) \vdash_\Sigma t : \beta$}
    \RightLabel{(i-abs)}
    \UnaryInfC{$\Gamma_l, \Gamma_i \vdash_\Sigma \lambda x. t : \alpha \to \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma_l, \Gamma_i \vdash_\Sigma t : \alpha \limp \beta$}
    \AxiomC{$\Delta_l, \Delta_i \vdash_\Sigma u : \alpha$}
    \RightLabel{(l-app)}
    \BinaryInfC{$(\Gamma_l, \Delta_l), (\Gamma_i, \Delta_i) \vdash_\Sigma (t\ u) : \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma_l, \Gamma_i \vdash_\Sigma t : \alpha \to \beta$}
    \AxiomC{$\emptyset, \Delta_i \vdash_\Sigma u : \alpha$}
    \RightLabel{(i-app)}
    \BinaryInfC{$\Gamma_l, (\Gamma_i, \Delta_i) \vdash_\Sigma (t\ u) : \beta$}
  \end{prooftree}
  \caption{\label{fig:type-judgments} Type judgment schemas of the
    well-typed lambda terms $\Lambda(\Sigma)$ for a signature $\Sigma =
    \mathopen{<}A, C, \tau\mathclose{>}$}
\end{figure}

The definition of a well-typed lambda term already gives us an
interesting combinatorial structure. To make this structure even more
useful, we often focus ourselves only on terms that have a specific
\emph{distinguished type}. Using this notion of a signature of typed
constants and some distinguished type, we can describe languages of,
e.g. tree-like (and by extension string-like), lambda terms.

\subsubsection{Example signature}
\label{sssec:example-sig}

Let us take a look at an example from
\cite{pogodalla2007generalizing}. We will start with a very simple
signature of string expressions with the concatenation operator,
$\Sigma_{String} = \mathopen{<}A_{String}, C_{String},
\tau_{String}\mathclose{>}$. Our signature will need only one atomic
type, $A_{String} = \{\textsc{String}\}$. Our constants will be the
wordforms of our example fragment ($every$, $some$, $man$, $woman$,
$loves$), the empty string $\epsilon$ and the string concatention
operator, $+$. $\tau_{String}$ assigns the type $\textsc{String}$ to all
the wordforms and to $\epsilon$ and the type $\textsc{String} \limp
\textsc{String} \limp \textsc{String}$\footnote{As a convention,
  whenever we omit parentheses in a type, we presume the $\limp$ and
  $\to$ type constructors always bind from the right, meaning that $a
  \to b \to c$ should be read as $a \to (b \to c)$.}
to the concatenation operator $+$.

Now we can look at some well-typed terms from the string domain
(i.e. elements of $\Lambda(\Sigma_{String})$). The term $t_1 =
some\ +\ woman$\footnote{Here we introduce another notational
  convention. The unadorned way of writing this term would be $t_1 =
  (+\ some)\ woman$. Firstly, we will allow ourselves to drop the
  parentheses, meaning that any string of expressions $f\ x\ y$ should
  be read as $(f\ x)\ y$. Secondly, we will admit a specific infix
  notation for some constants which denote functions of two
  arguments. This will then let us write $x\ +\ y$ instead of
  $+\ x\ y$.} denotes the concatenation of $some$ and $woman$, which is
the phrase $some\ woman$; its type is $\textsc{String}$. A term such as
$t_2 = \lambda^{\circ} x. x\ +\ man$ denotes a function that appends the
string $man$ to its argument; its type is $\textsc{String} \limp
\textsc{String}$. If we restrict the set $\Lambda(\Sigma_{String})$ only
to terms having the type $\textsc{String}$, we get the set of
expressions which denote strings (a set that contains terms like $t_1$
but not $t_2$).

\subsubsection{Lexicons}

The idea of a signature is coupled with that of a \textbf{lexicon},
which is a mapping between two different signatures (mapping the
constants of one into well-typed terms of the other). Formally speaking,
a lexicon $\mathcal{L}$ from a signature $\Sigma_1 = \mathopen{<}A_1, C_1,
\tau_1\mathclose{>}$ (which we call the abstract signature) to a
signature $\Sigma_2 = \mathopen{<}A_2, C_2, \tau_2\mathclose{>}$ (which
we call the object signature) is a pair $\mathopen{<}F, G\mathclose{>}$
such that:
\begin{itemize}
\item $G$ is a mapping from $C_1$ to $\Lambda(\Sigma_2)$ assigning to
  every constant of the abstract signature a term in the object
  signature, which can be understood as its
  interpretation/implementation/realization.
\item $F$ is a mapping from $A_1$ to $\mathcal{T}(A_2)$ which links the
  abstract-level types with the object-level types that they realized
  in.
\item $F$ and $G$ are compatible, meaning that for any $c \in C_1$, we
  have $\vdash_{\Sigma_2} G(c) : \hat{F}(\tau_1(c))$ (we will be using
  $\hat{F}$ and $\hat{G}$ to refer to the homomorphic extensions of $F$
  and $G$ to $\mathcal{T}(A_1)$ and $\Lambda(\Sigma_1)$ respectively).
\end{itemize}

An abstract categorial grammar for us will then be just a collection of
signatures with their distinguished types and lexicons connecting these
signatures. A common pattern will have us using two signatures for the
surface forms of utterances (strings) and their logical forms (logical
propositions) and an abstract signature which is connected to both of
the object signatures via lexicons. Parsing is then just a matter of
inverting the surface lexicon to get the abstract term and then applying
to it the logical lexicon. Generation is symmetric, we simply invert the
logical lexicon and apply the surface lexicon.

\subsubsection{Example lexicon}
\label{sssec:example-lex}

\newcommand{\synt}[1]{C_{\textrm{#1}}}

To illustrate the ideas of a lexicon and an abstract categorial grammar,
we will expand our example from \ref{sssec:example-sig}. We will
consider another signature, $\Sigma_{Synt}$, which will describe the
syntax of quantified noun phrases in the style of Montague
\cite{montague1973proper}. The atomic types $A_{Synt}$ will consist of
the types $np$, $n$ and $s$. Our constants, $C_{Synt} = \{\synt{every},
\synt{some}, \synt{love}, \synt{man}, \synt{woman}\}$, will have the
following types, as predicted by $\tau_{Synt}$:
\begin{itemize}
\item $\tau_{Synt}(\synt{every}) = \tau_{Synt}(\synt{some}) = n \limp
  ((np \limp s) \limp s)$
\item $\tau_{Synt}(\synt{love}) = np \limp np \limp s$
\item $\tau_{Synt}(\synt{man}) = \tau_{Synt}(\synt{woman}) = n$
\end{itemize}

Let us explore some terms from $\Lambda(\Sigma_{Synt})$. $t_3 =
\synt{some}\ \synt{woman}$ has type $(np \limp s) \limp s$, which is the
type that serves to describe quantified noun phrases in our
example. Terms of this type expect a verb phrase (or some other
predicate) of type $np \limp s$, and can yield a sentence of type $s$.

Now we will see how this approach can handle transitive verbs. In $t_4 =
\lambda^{\circ} x.\ (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
y.\ \synt{love}\ x\ y)$, which is a term of type $np \limp s$
representing the verb phrase \emph{loves some woman}, we first take the
expression $\synt{love}\ x\ y$ (\emph{$x$ loves $y$}, type $s$) and we
abstract over $y$ to get the predicate $\lambda^{\circ}
y.\ \synt{love}\ x\ y$ (\emph{is loved by $x$}, type $np \limp s$). By
applying the quantified noun phrase $t_3$ to this term, we get
$(\synt{some}\ \synt{woman})\ (\lambda^{\circ} y.\ \synt{love}\ x\ y)$
(\emph{$x$ loves some woman}, type $s$). Finally, by abstracting over
$x$, we get the verb phrase $t_4$.

$t_5 = (\synt{every}\ \synt{man})\ (\lambda^{\circ}
x.\ (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
y.\ \synt{love}\ x\ y))$ is the result of applying the quantified noun
phrase $\synt{every}\ \synt{man}$ (of type $(np \limp s) \limp s$) to
$t_4$. What we get is a term of type $s$ which describes the sentence
\emph{every man loves some woman}. Note that as before, we can restrict
the set $\Lambda(\Sigma_{Synt})$ to terms having the distinguished type
$s$ and we will arrive at the set of terms which describe sentences
(this set will include $t_5$, but not $t_3$ or $t_4$).

In the example above, I have been presuming some kind of implied
connection between the terms of $\Lambda(\Sigma_{Synt})$ and phrases of
some natural language, strings. Indeed, while we have defined a system
of structures which describe the quantification and predicate structures
of a microscopic fragment of English and a system for talking about
strings in concatenation, we have not given any explicit link between
the two. It is at this moment that we will introduce a lexicon to link
these two levels of description.

Our lexicon $\mathcal{L}_{syntax}$ will map the constants of
$\Sigma_{Synt}$, our abstract signature, into terms from
$\Lambda(\Sigma_{String})$, our object signature. If we view terms of
$\Lambda(\Sigma_{Synt})$ as abstract computations, the lexicon will
instantiate this abstraction by providing an implementation for the
constants of $\Sigma_{Synt}$. This way we can map an abstract
computation representing a phrase into a more specific object
computation that computes the string representing the phrase.

So, how does our lexicon $\mathcal{L}_{syntax}$ look like? It will map
all the atomic types of $\Sigma_{Synt}$ into $\textsc{String}$, which we
can write as $\mathcal{L}_{syntax}(n) = \mathcal{L}_{syntax}(np) =
\mathcal{L}_{syntax}(s)$\footnote{Whenever we talk about some lexicon,
  we will use the lexicon itself, which is formally a pair of mappings
  $\mathopen{<}F, G\mathclose{>}$, to mean either $F$, $G$ or their
  homomorphic extensions $\hat{F}$ and $\hat{G}$ depending on whether
  the argument is an atomic type, a constant, a complex type or a term
  respectively.}. From that, we can use the homomorphic extension of our
type mapping to arrive at the object-level interpretations of complex
types, such as the type of predicates, $np \limp s$, which becomes a
unary string function, $\textsc{String} \limp \textsc{String}$, or the
type of quantified noun phrases, $(np \limp s) \limp s$, which becomes a
higher-order string function, $(\textsc{String} \limp \textsc{String})
\limp \textsc{String}$. With the types out of the way, we can give the
interpretation of the individual constants of $\Sigma_{Synt}$:
\begin{itemize}
\item $\mathcal{L}_{syntax}(\synt{every}) = \lambda^{\circ} x
  R.\ R\ (every\ +\ x)$
\item $\mathcal{L}_{syntax}(\synt{some}) = \lambda^{\circ} x
  R.\ R\ (some\ +\ x)$
\item $\mathcal{L}_{syntax}(\synt{love}) = \lambda^{\circ} x
  y.\ x\ +\ loves\ +\ y$
\item $\mathcal{L}_{syntax}(\synt{man}) = man$
\item $\mathcal{L}_{syntax}(\synt{woman}) = woman$
\end{itemize}

We can now go back to our examples from $\Lambda(\Sigma_{Synt})$ and
look at what they look like after applying the lexicon to them.
\begin{itemize}
\item $t_3 = \synt{some}\ \synt{woman}$ \\ Mapping this term using the
  lexicon and $\beta$-reducing gives us $\mathcal{L}_{syntax}(t_3)
  =_{\beta} \lambda^{\circ} R.\ R\ (some\ +\ woman)$, which is a
  type-raised version of the string \emph{some woman}.
\item $t_4 = \lambda^{\circ}
  x.\ (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
  y.\ \synt{love}\ x\ y)$ \\ Our verb phrase ends up being mapped onto
  a function which appends the phrase \emph{loves some woman} to its
  argument, $\mathcal{L}_{syntax}(t_4) =_{\beta} \lambda^{\circ}
  x.\ x\ +\ loves\ +\ some\ +\ woman$
\item $t_5 = (\synt{every}\ \synt{man})\ (\lambda^{\circ}
  x.\ (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
  y.\ \synt{love}\ x\ y))$ \\ Finally, the sentence (type $s$) is mapped
  into a simple $\textsc{String}$, $\mathcal{L}_{syntax}(t_5) =_{\beta}
  every\ +\ man\ +\ loves\ +\ some\ +\ woman$.
\end{itemize}

Now we have enough machinery in play to define the set of strings which
form our fragment of English. We will consider the \emph{abstract
  language} generated by our signature $\Sigma_{Synt}$ and the
distinguished type $s$, that is, elements of $\Lambda(\Sigma_{Synt})$
having type $s$. Then we can define the \emph{object language}, which is
the image of the abstract language when transformed using the
lexicon. The object language contains strings, terms from
$\Lambda(\Sigma_{String})$ having the type $\textsc{String}$. However,
the object language does not contain all such terms, but only those
object terms, for which there exists a term in the abstract language
such that its image given by the lexicon yields the same object term.

\subsubsection{Example semantic lexicon}

One perk of working with abstract categorial grammars is that one
abstract term can be interpreted multiple ways, using different
lexicons. In the previous section, we considered the abstract terms of
the $\Sigma_{Synt}$ signature and interpreted them as computations on
strings. Now we will take the same abstract terms, but try to intepret
them as computations on entities and truth values.

We will introduce a new signature, $\Sigma_{Sem}$, for our semantic
computations. The atomic types $A_{Sem}$ will consist of a type for
entities, $e$, and a type for truth values, $t$. The constants $C_{Sem}$
will contain atomic predicates from our fragment, $\textbf{man}$,
$\textbf{woman}$ and $\textbf{love}$, logical connectives, $\Rightarrow$
and $\land$, and quantifiers, $\forall$ and $\exists$. Their types as
given by $\tau_{Sem}$ are:
\begin{itemize}
\item $\tau_{Sem}(\textbf{man}) = \tau_{Sem}(\textbf{woman}) = e \limp
  t$
\item $\tau_{Sem}(\textbf{love}) = e \limp e \limp t$
\item $\tau_{Sem}(\Rightarrow) = \tau_{Sem}(\land) = t \limp t \limp t$
\item $\tau_{Sem}(\forall) = \tau_{Sem}(\exists) = (e \to t) \limp t$
\end{itemize}

Now to see how this signature connects to our abstract signature
$\Sigma_{Synt}$, we will give a lexicon from the latter to the
former. First, we give the type mappings. $\mathcal{L}_{sem}(n) = e
\limp t$ meaning that nouns will yield boolean functions of one argument
(unary \emph{predicates} in functional programming
terminology). $\mathcal{L}_{sem}(np) = e$, simple noun phrases will be
mapped into terms which yield some entity. Finally,
$\mathcal{L}_{sem}(s) = t$, sentences will be assigned terms which
compute their truthiness. Let us see how this plan is achieved in the
$\mathcal{L}_{sem}$ lexicon.
\begin{itemize}
\item $\mathcal{L}_{sem}(\synt{every}) = \lambda^{\circ} P Q.\ \forall
  x.\ ((P\ x) \Rightarrow (Q\ x))$\footnote{We will be using $\forall
    x.\ M$ as a shortcut for $\forall\ (\lambda x.\ M)$ and $\exists
    x.\ M$ for $\exists\ (\lambda x.\ M)$.}
\item $\mathcal{L}_{sem}(\synt{some}) = \lambda^{\circ} P Q.\ \exists
  x.\ ((P\ x) \land (Q\ x))$
\item $\mathcal{L}_{sem}(\synt{love}) = \textbf{love}$
\item $\mathcal{L}_{sem}(\synt{man}) = \textbf{man}$
\item $\mathcal{L}_{sem}(\synt{woman}) = \textbf{woman}$
\end{itemize}

Let us now consider the example terms of the $\Sigma_{Synt}$ signature
from \ref{sssec:example-lex}. $t_3 = \synt{some}\ \synt{woman}$ will be
interpreted as $\mathcal{L}_{sem}(t_3) =_{\beta} \lambda^{\circ}
Q.\ \exists x.\ ((\textbf{woman}\ x) \land (Q\ x))$ having type $(e \limp
t) \limp t$. It is a function that expects some predicate function and
tests whether that predicate holds at least for some woman (some entity
for which the $\textbf{woman}$ predicate holds).

In $t_4 = \lambda^{\circ}
x.\ (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
y.\ \synt{love}\ x\ y)$, we get $\mathcal{L}_{sem}(t_4) =_{\beta}
\lambda^{\circ} x.\ \exists y.\ ((\textbf{woman}\ y) \land
(\textbf{love}\ x\ y))$ of type $e \limp t$. What we have here is a
predicate function testing whether the argument entity loves some woman.

Finally, in $t_5 = (\synt{every}\ \synt{man})\ (\lambda^{\circ}
x.\ (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
y.\ \synt{love}\ x\ y))$, we have $\mathcal{L}_{sem}(t_5) =_{\beta}
\forall x.\ ((\textbf{man}\ x) \Rightarrow (\exists
y.\ ((\textbf{woman}\ y) \land (\textbf{love}\ x\ y))))$ of type
$t$. This computation yields a truth value, which tells us whether the
proposition that every man loves some woman is true given some universe
that the quantifiers range over and some values of the $\textbf{man}$,
$\textbf{woman}$ and $\textbf{love}$ predicates. The term itself, in its
$\beta$-normal form, can serve as our semantic representation for the
proposition \emph{every man loves some woman}.

We will finish our exposition of abstract categorial grammars by
highlighting a prominent feature of the example grammar we have just
described and that is its treatment of scope ambiguity. You might have
noticed that $t_5 = (\synt{every}\ \synt{man})\ (\lambda^{\circ}
x.\ (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
y.\ \synt{love}\ x\ y))$ is not the only term in
$\Lambda(\Sigma_{Synt})$ for which $\mathcal{L}_{syntax}(t_5) =_{\beta}
every\ +\ man\ +\ loves\ +\ some\ +\ woman$. We could just as well take $t_6
= (\synt{some}\ \synt{woman})\ (\lambda^{\circ}
y.\ (\synt{every}\ \synt{man})\ (\lambda^{\circ}
x.\ \synt{love}\ x\ y))$ which is not $\beta$-equivalent to $t_5$. When
we apply our syntactic lexicon to $t_6$, we see that we end up with an
expression denoting the same string, $\mathcal{L}_{syntax}(t_6)
=_{\beta} every\ +\ man\ +\ loves\ +\ some\ +\ woman$.

However, when we consider a different interpretation for the abstract
constants in $\Sigma_{Synt}$, for example our semantic lexicon
$\mathcal{L}_{sem}$, we can see the differences rise to surface.

$$
\mathcal{L}_{sem}(t_5) =_{\beta} \forall x.\ ((\textbf{man}\ x)
\Rightarrow (\exists y.\ ((\textbf{woman}\ y) \land
(\textbf{love}\ x\ y))))
$$
$$
\mathcal{L}_{sem}(t_6) =_{\beta} \exists y.\ ((\textbf{woman}\ y)
\land (\forall x.\ ((\textbf{man}\ x) \Rightarrow
(\textbf{love}\ x\ y))))
$$

When we try to parse some sentence using abstract categorial grammars,
we are trying to find the abstract terms which upon transformation by
the lexicon and $\beta$-reduction yield the sentence encoded in some
object term. This is basically trying to invert the lexicon function,
modulo $\beta$-equivalence. It is therefore no surprise then that we can
find multiple abstract terms which all map to the same string term but
which can be mapped to distinct terms using the semantic lexicon. It is
this mechanism that enables abstract categorial grammars to handle
ambiguity. When we reverse the scenario and go from semantic
representations to strings, the same situation can occur (more than one
abstract term having the same semantics but different surface strings)
and this is what enables paraphrases.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
